{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T16:24:27.079545Z",
     "start_time": "2022-01-24T16:24:16.607994Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(4)\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import rasterio as rio\n",
    "from copy import deepcopy\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam, Adam\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor as ANN\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNN\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.colors as colors\n",
    "from rasterio.plot import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-24T16:23:08.668Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_random_seed(x):\n",
    "    tf.random.set_seed(x) # Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    np.random.seed(x)     # Set the `numpy` pseudo-random generator at a fixed value\n",
    "    random.seed(x)        # Set the `python` built-in pseudo-random generator at a fixed value      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-24T16:23:09.703Z"
    }
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Reproducibility is a Problem when using parallel processing  (n_jobs = 1)#\n",
    "############################################################################ \n",
    "seed = 4\n",
    "set_random_seed(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-24T16:23:10.426Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(learn_rate=0.01, units1=14,units2=12,activ_func1='sigmoid',activ_func2='sigmoid',activ_func3='sigmoid'):    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units1, kernel_initializer='uniform', activation=activ_func1, input_shape=(Nfeatures,))) \n",
    "    model.add(Dense(units2, kernel_initializer='uniform', activation=activ_func2))                           \n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation=activ_func3))\n",
    "    optimizer = Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adam\")\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T07:02:42.836497Z",
     "start_time": "2022-01-21T07:02:42.804351Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPixelValue(array,idx1,idx2,idx3):\n",
    "    return array[idx1,idx2,idx3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T07:22:23.798023Z",
     "start_time": "2022-01-21T07:03:10.883615Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [19:12<00:00, 76.85s/it]\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# Prepare excel files containing all pixel values of best 2 S2 (including missing values = -99)\n",
    "# Seperate None and valid pixels into 2 excel files\n",
    "##########\n",
    "# Without GF\n",
    "##########\n",
    "\n",
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\preparedInputData'\n",
    "gf_folders = ['withoutGF']\n",
    "\n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    subdir2 = os.path.join(maindir2,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' in fileName]\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]        \n",
    "        img = rio.open(os.path.join(subdir1,file1)) # start by reading all layers\n",
    "        arr = img.read()\n",
    "        # Rank S2 scenes based on n° KP \n",
    "        indices = [i for i in range(1,36,9)]\n",
    "        nb_KP = []\n",
    "        for i in indices:\n",
    "            temp_copy = deepcopy(arr[i])\n",
    "            temp_copy[temp_copy==-99]=9.96921e+36\n",
    "            nb_KP.append(len(np.argwhere(temp_copy<=1e+36).tolist()))        \n",
    "        df1 = pd.DataFrame({'indices':indices,'nb_KP':nb_KP})\n",
    "        df1.sort_values('nb_KP', inplace=True)  # order based on nb_KP and make changes to df permanent (order from worst to best)\n",
    "        df1.reset_index(drop=True, inplace=True) # Drop old index and make changes to df permanent\n",
    "        \n",
    "        # Select reflectance layers associated with best 2 images\n",
    "        name = 'Pixels_From_Best_2_S2_'+file1[7:15]\n",
    "        l = list(df1[2:]['indices']) # get the best 2 S2\n",
    "        # Create a new stacked array of layers to be used (from which we will extract coord of KP, UP, None pixels)\n",
    "        arr_temp = np.expand_dims(arr[0], axis=0)\n",
    "        for k1 in l:\n",
    "            for k2 in range(k1,k1+9):\n",
    "                arr_temp = np.append(arr_temp,np.expand_dims(arr[k2], axis=0),axis=0) # get 19 layers (1st layer is turbidity + 18 layers of best S2 images and associated combinations of bands )\n",
    "        \n",
    "        # Get all possible pixel coordinates for valid or none pixel values (=9.96921e+36)\n",
    "        idX = []\n",
    "        idY = [] \n",
    "        idX_none = []\n",
    "        idY_none = []\n",
    "        for idx in range(arr_temp.shape[1]):    # get all pixel coordinates\n",
    "            for idy in range(arr_temp.shape[2]):\n",
    "                if arr_temp[1,idx,idy] > 1e+36 or arr_temp[10,idx,idy] > 1e+36: # Exclude if pixel is none in one of the layers\n",
    "                    idX_none.append(idx)                                            \n",
    "                    idY_none.append(idy)\n",
    "                else:\n",
    "                    if arr_temp[1,idx,idy] ==-99 and arr_temp[10,idx,idy] ==-99: # Exclude if 2 S2 is UP\n",
    "                        idX_none.append(idx)                                            \n",
    "                        idY_none.append(idy)\n",
    "                    else:                       # Save if 1 S2 is KP\n",
    "                        idX.append(idx)                                      # 1: 1st best S2 image # 10: 2nd best S2 image \n",
    "                        idY.append(idy)                   \n",
    "        \n",
    "        # Store all pixel values (!=none) in an empty df            \n",
    "        rows = ['L'+str(index) for index in range(len(arr_temp))]\n",
    "        columns = [index for index in range(len(idX))]\n",
    "        results = pd.DataFrame(index=rows, columns=columns)\n",
    "        data = [] # It is recommended to collect data in a list of lists and then assign it to a df (Than modifying a df each iteration => time costly and prone to error of dtypes)\n",
    "        for idxLayer in range(len(arr_temp)):\n",
    "            pixelValues = Parallel(n_jobs=-1)(delayed(getPixelValue)(arr_temp,idxLayer,idX[k],idY[k]) for k in range(len(idX)))\n",
    "            data.append(pixelValues)\n",
    "        results = pd.DataFrame(data, index=rows, columns=columns).T\n",
    "        results.insert(loc=0, column='idx', value=idX)   # Add coordinates to df (while specifying position)\n",
    "        results.insert(loc=1, column='idy', value=idY)  \n",
    "        # Store all pixel values (==none) in an empty df            \n",
    "        results_none = pd.DataFrame({'idx_none':idX_none, 'idy_none':idY_none})\n",
    "        \n",
    "        # Export as excel files\n",
    "        os.makedirs(subdir2, exist_ok=True)\n",
    "        outputdir = os.path.join(subdir2, name+'.xlsx')\n",
    "        results.to_excel(outputdir, encoding='utf-8')\n",
    "        \n",
    "        outputdir2 = os.path.join(subdir2,'coordsNonePixelValues'+str(n)+'.xlsx') \n",
    "        results_none.to_excel(outputdir2, encoding='utf-8', index=False) # The coords of none pixel values are the same # save them 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T07:39:48.087769Z",
     "start_time": "2022-01-21T07:22:29.951882Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [17:18<00:00, 69.21s/it]\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# Prepare excel files containing all pixel values of best 2 S2 (including missing values = -99)\n",
    "# Seperate None and valid pixels into 2 excel files\n",
    "##########\n",
    "# With GF\n",
    "##########\n",
    "\n",
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\preparedInputData'\n",
    "gf_folders = ['withGF']\n",
    "\n",
    "for gf_folder in gf_folders:\n",
    "    subdir0 = os.path.join(maindir1,'withoutGF','France')    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    subdir2 = os.path.join(maindir2,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' in fileName]\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]  \n",
    "        arr0 = rio.open(os.path.join(subdir0,file1)).read()\n",
    "        img = rio.open(os.path.join(subdir1,file1)) # start by reading all layers\n",
    "        arr = img.read()\n",
    "        # Rank S2 scenes based on n° KP \n",
    "        indices = [i for i in range(1,36,9)]\n",
    "        nb_KP = []\n",
    "        for i in indices:\n",
    "            temp_copy = deepcopy(arr0[i])\n",
    "            temp_copy[temp_copy==-99]=9.96921e+36\n",
    "            nb_KP.append(len(np.argwhere(temp_copy<=1e+36).tolist()))        \n",
    "        df1 = pd.DataFrame({'indices':indices,'nb_KP':nb_KP})\n",
    "        df1.sort_values('nb_KP', inplace=True)  # order based on nb_KP and make changes to df permanent (order from worst to best)\n",
    "        df1.reset_index(drop=True, inplace=True) # Drop old index and make changes to df permanent\n",
    "        \n",
    "        # Select reflectance layers associated with best 2 images\n",
    "        name = 'Pixels_From_Best_2_S2_'+file1[7:15]\n",
    "        l = list(df1[2:]['indices']) # get the best 2 S2\n",
    "        # Create a new stacked array of layers to be used (from which we will extract coord of KP, UP, None pixels)\n",
    "        arr_temp = np.expand_dims(arr[0], axis=0)\n",
    "        for k1 in l:\n",
    "            for k2 in range(k1,k1+9):\n",
    "                arr_temp = np.append(arr_temp,np.expand_dims(arr[k2], axis=0),axis=0) # get 19 layers (1st layer is turbidity + 18 layers of best S2 images and associated combinations of bands )\n",
    "        \n",
    "        # Get all possible pixel coordinates for valid or none pixel values (=9.96921e+36)\n",
    "        idX = []\n",
    "        idY = [] \n",
    "        idX_none = []\n",
    "        idY_none = []\n",
    "        for idx in range(arr_temp.shape[1]):    # get all pixel coordinates\n",
    "            for idy in range(arr_temp.shape[2]):\n",
    "                if arr_temp[1,idx,idy] > 1e+36 or arr_temp[10,idx,idy] > 1e+36: # Exclude if pixel is none in one of the layers\n",
    "                    idX_none.append(idx)                                            \n",
    "                    idY_none.append(idy)\n",
    "                else:\n",
    "                    idX.append(idx)                                      # 1: 1st best S2 image # 10: 2nd best S2 image \n",
    "                    idY.append(idy)                   \n",
    "        \n",
    "        # Store all pixel values (!=none) in an empty df            \n",
    "        rows = ['L'+str(index) for index in range(len(arr_temp))]\n",
    "        columns = [index for index in range(len(idX))]\n",
    "        results = pd.DataFrame(index=rows, columns=columns)\n",
    "        data = [] # It is recommended to collect data in a list of lists and then assign it to a df (Than modifying a df each iteration => time costly and prone to error of dtypes)\n",
    "        for idxLayer in range(len(arr_temp)):\n",
    "            pixelValues = Parallel(n_jobs=-1)(delayed(getPixelValue)(arr_temp,idxLayer,idX[k],idY[k]) for k in range(len(idX)))\n",
    "            data.append(pixelValues)\n",
    "        results = pd.DataFrame(data, index=rows, columns=columns).T\n",
    "        results.insert(loc=0, column='idx', value=idX)   # Add coordinates to df (while specifying position)\n",
    "        results.insert(loc=1, column='idy', value=idY)  \n",
    "        # Store all pixel values (==none) in an empty df            \n",
    "        results_none = pd.DataFrame({'idx_none':idX_none, 'idy_none':idY_none})\n",
    "        \n",
    "        # Export as excel files\n",
    "        os.makedirs(subdir2, exist_ok=True)\n",
    "        outputdir = os.path.join(subdir2, name+'.xlsx')\n",
    "        results.to_excel(outputdir, encoding='utf-8')\n",
    "        \n",
    "        outputdir2 = os.path.join(subdir2,'coordsNonePixelValues'+str(n)+'.xlsx') \n",
    "        results_none.to_excel(outputdir2, encoding='utf-8', index=False) # The coords of none pixel values are the same # save them 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T08:01:31.554439Z",
     "start_time": "2022-01-21T07:39:54.184039Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [10:42<00:00, 42.82s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [10:55<00:00, 43.67s/it]\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# generate new turbidity maps #\n",
    "###############################\n",
    "\n",
    "# predict turbidity using all training dataset\n",
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\preparedInputData'\n",
    "maindir3 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "    \n",
    "for gf_folder in gf_folders:    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    subdir2 = os.path.join(maindir2,gf_folder,'France')\n",
    "    subdir3 = os.path.join(maindir3,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]        \n",
    "\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        for file2 in os.listdir(os.path.join(subdir1,file1)): \n",
    "            if 'Best_2' in file2:\n",
    "                ############### Read all training dataset (without splitting) ###############\n",
    "                # first train the model with the previously prepared training set. Then, apply the model to predict turbidity in whole study area #\n",
    "                excel_file = pd.read_excel(os.path.join(subdir1,file1,file2)) # step0: Read and split data\n",
    "                y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                features = ['L'+str(i) for i in range(1,len(excel_file.columns)-3)]\n",
    "                X = excel_file.values                \n",
    "#                 y = MinMaxScaler().fit_transform(y) # Data Normalization is not necessary for random forests\n",
    "#                 X = MinMaxScaler().fit_transform(X) # This will save us the time of invert normalization afterwards\n",
    "\n",
    "                Nfeatures = X.shape[1]\n",
    "                y = y.ravel() # flatten to 1d array # data is in a column format while it expected it in a row.\n",
    "                ############### Read all pixel values in 2 S2 images to predict corresponding turbidity values ###############\n",
    "                excel_file2 = pd.read_excel(os.path.join(subdir2,file2)) \n",
    "                idx = np.array(excel_file2['idx'].values,dtype=np.float).reshape(-1,1)                        \n",
    "                idy = np.array(excel_file2['idy'].values,dtype=np.float).reshape(-1,1)\n",
    "                excel_file2.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                features2 = ['L'+str(i) for i in range(1,len(excel_file2.columns)-3)]\n",
    "                S2_values = excel_file2.values                \n",
    "#                 S2_values = MinMaxScaler().fit_transform(S2_values) # Data Normalization\n",
    "                ############### Predict turbidity using RF ()############### \n",
    "                model = RFR(n_estimators=500, max_features=int(len(features)/3.0), max_depth=25, random_state=seed)                    \n",
    "                model.fit(X, y)\n",
    "                y_pred = model.predict(S2_values)\n",
    "                results = pd.DataFrame({'idx':idx.ravel(), 'idy':idy.ravel(), 'predTur':y_pred.ravel()})\n",
    "\n",
    "                # step7: Export as excel files\n",
    "                outputdir2 = os.path.join(subdir3,'Tur_'+file1+'.xlsx')\n",
    "                results.to_excel(outputdir2, encoding='utf-8', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T08:08:58.944764Z",
     "start_time": "2022-01-21T08:01:37.606147Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [03:47<00:00, 14.19s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [03:34<00:00, 13.39s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "    \n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    subdir2 = os.path.join(maindir2,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'None' in fileName]\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        # Read file: coordsNonePixelValues\n",
    "        excel_file1 = pd.read_excel(os.path.join(subdir1,file1)) # step0: Read and split data\n",
    "        idx_temp1 = list(excel_file1['idx_none'])\n",
    "        idy_temp1 = list(excel_file1['idy_none'])\n",
    "        noneValues = []\n",
    "        for i in range(excel_file1.shape[0]):\n",
    "            noneValues.append(9.96921e+36)\n",
    "        \n",
    "        df = pd.DataFrame({'idx':idx_temp1, 'idy':idy_temp1, 'predTur':noneValues})\n",
    "        # step7: Export as excel files\n",
    "        outputdir2 = os.path.join(subdir2,file1)\n",
    "        df.to_excel(outputdir2, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T08:19:39.548284Z",
     "start_time": "2022-01-21T08:09:04.967483Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [05:17<00:00, 21.17s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [05:16<00:00, 21.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# Read predicted turbidity pixel values and add the none values to it\n",
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "    \n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1, gf_folder, 'France')\n",
    "    subdir2 = os.path.join(maindir2, gf_folder, 'France')\n",
    "    files_temp1 = [fileName for fileName in os.listdir(subdir1) if ('Tur' in fileName)and('tiff' not in fileName)]  \n",
    "\n",
    "    for n in tqdm(range(len(files_temp1))):\n",
    "        file1 = files_temp1[n]\n",
    "        excel_file1 = pd.read_excel(os.path.join(subdir1,file1))\n",
    "        idx_temp1 = list(excel_file1['idx'])\n",
    "        idy_temp1 = list(excel_file1['idy'])\n",
    "        predTur_temp1 = list(excel_file1['predTur'])\n",
    "        \n",
    "        file2 = 'coordsNonePixelValues'+str(n)+'.xlsx'        \n",
    "        excel_file2 = pd.read_excel(os.path.join(subdir1,file2))        \n",
    "        idx_temp2 = list(excel_file2['idx'])\n",
    "        idy_temp2 = list(excel_file2['idy'])\n",
    "        predTur_temp2 = list(excel_file2['predTur'])\n",
    "        \n",
    "        idx = idx_temp1+idx_temp2\n",
    "        idy = idy_temp1+idy_temp2\n",
    "        predTur = predTur_temp1+predTur_temp2\n",
    "\n",
    "        results = pd.DataFrame({'idx':idx, 'idy':idy, 'predTur':predTur})\n",
    "        results.sort_values(by=['idx', 'idy'], ascending=True, inplace=True) # Sort Values by idx then by idy\n",
    "\n",
    "        rowsList = results['idx']\n",
    "        colList = results['idy']\n",
    "        turList = results['predTur']\n",
    "\n",
    "        file3 = file1[4:12]        \n",
    "        img = rio.open(os.path.join(subdir2,'merged_'+file3+'.tiff')) # start by reading all layers\n",
    "        arr = img.read()\n",
    "                \n",
    "        ######## Update Array ########  \n",
    "        # Export as images \n",
    "        temp_copy1 = deepcopy(arr[0]) # retain layer as actual turbidity\n",
    "        outputdir1 = os.path.join(subdir1, 'actual_'+file1[:-5]+'.tiff')\n",
    "        with rio.open(outputdir1,'w',driver='Gtiff', width=img.width, height=img.height, \n",
    "                            count=1,crs=img.crs,transform=img.transform, dtype='float32', nodata=9.96921e+36) as newImg:\n",
    "            newImg.write(temp_copy1,1)\n",
    "            newImg.close()\n",
    "        \n",
    "        temp_copy2 = deepcopy(arr[0]) # to be filled with predicted turbidity\n",
    "        for item in range(len(rowsList)):\n",
    "            temp_copy2[int(rowsList[item]),int(colList[item])] = turList[item]\n",
    "        outputdir2 = os.path.join(subdir1, 'predicted_'+file1[:-5]+'.tiff')\n",
    "        with rio.open(outputdir2,'w',driver='Gtiff', width=img.width, height=img.height, \n",
    "                            count=1,crs=img.crs,transform=img.transform, dtype='float32', nodata=9.96921e+36) as newImg:\n",
    "            newImg.write(temp_copy2,1)\n",
    "            newImg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T16:24:45.515765Z",
     "start_time": "2022-01-24T16:24:45.489031Z"
    }
   },
   "outputs": [],
   "source": [
    "## https://gist.github.com/bshishov/5dc237f59f019b26145648e2124ca1c9\n",
    "\n",
    "EPSILON = 1e-10\n",
    "\n",
    "def _error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Simple error \"\"\"\n",
    "    return actual - predicted\n",
    "def _absolute_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" absolute error \"\"\"\n",
    "    return abs(actual - predicted)\n",
    "def _percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Percentage error\n",
    "    Note: result is NOT multiplied by 100\n",
    "    \"\"\"\n",
    "    return (actual - predicted) / (actual + EPSILON)\n",
    "def error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Simple error \"\"\"\n",
    "    return np.mean(_error(actual, predicted))\n",
    "def percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Percentage error\n",
    "    Note: result is NOT multiplied by 100\n",
    "    \"\"\"\n",
    "    return np.mean((actual - predicted)/(actual + EPSILON))\n",
    "    \n",
    "def mse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Mean Squared Error \"\"\"\n",
    "    return np.mean(np.square(_error(actual, predicted)))\n",
    "def mdape(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Median Absolute Percentage Error\n",
    "    \"\"\"\n",
    "    return np.median(np.abs(_percentage_error(actual, predicted)))\n",
    "def R2_score(actual: np.ndarray, predicted: np.ndarray):\n",
    "    return r2_score(actual, predicted)\n",
    "\n",
    "METRICS = {\n",
    "    'mse': mse,\n",
    "    'mdape': mdape, # less affected by outliers\n",
    "    '_error':_error,\n",
    "    '_percentage_error':_percentage_error,\n",
    "    'error':error,\n",
    "    'percentage_error':percentage_error,\n",
    "    'R2_score':R2_score,\n",
    "}\n",
    "\n",
    "def evaluate(actual: np.ndarray, predicted: np.ndarray, metrics=('mse', 'mdape', '_error','_percentage_error', 'error','percentage_error','R2_score')):\n",
    "    results = {}\n",
    "    for name in metrics:\n",
    "        try:\n",
    "            results[name] = METRICS[name](actual, predicted)\n",
    "        except Exception as err:\n",
    "            results[name] = np.nan\n",
    "            print('Unable to compute metric {0}: {1}'.format(name, err))\n",
    "    return results\n",
    "\n",
    "def evaluate_all(actual: np.ndarray, predicted: np.ndarray):\n",
    "    return evaluate(actual, predicted, metrics=set(METRICS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T08:20:27.423372Z",
     "start_time": "2022-01-21T08:19:51.861970Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:17<00:00,  1.18s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:17<00:00,  1.19s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gf</th>\n",
       "      <th>d</th>\n",
       "      <th>err</th>\n",
       "      <th>errP</th>\n",
       "      <th>MdAPE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190121</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>-3.57</td>\n",
       "      <td>8.76</td>\n",
       "      <td>0.1512</td>\n",
       "      <td>97.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190211</td>\n",
       "      <td>0.2876</td>\n",
       "      <td>-62.45</td>\n",
       "      <td>45.28</td>\n",
       "      <td>10.9013</td>\n",
       "      <td>14.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190311</td>\n",
       "      <td>-0.0018</td>\n",
       "      <td>-4.19</td>\n",
       "      <td>9.90</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>71.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190411</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>-3.37</td>\n",
       "      <td>11.20</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>88.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190511</td>\n",
       "      <td>-0.0063</td>\n",
       "      <td>-3.24</td>\n",
       "      <td>8.26</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>94.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190611</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>-19.19</td>\n",
       "      <td>19.14</td>\n",
       "      <td>1.6930</td>\n",
       "      <td>33.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190711</td>\n",
       "      <td>-0.0088</td>\n",
       "      <td>-18.09</td>\n",
       "      <td>19.40</td>\n",
       "      <td>0.4316</td>\n",
       "      <td>31.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190811</td>\n",
       "      <td>-0.0265</td>\n",
       "      <td>-14.74</td>\n",
       "      <td>14.37</td>\n",
       "      <td>2.0530</td>\n",
       "      <td>7.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190911</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>-2.83</td>\n",
       "      <td>7.74</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>54.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20191011</td>\n",
       "      <td>-0.1051</td>\n",
       "      <td>-17.55</td>\n",
       "      <td>16.70</td>\n",
       "      <td>0.1093</td>\n",
       "      <td>87.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20191111</td>\n",
       "      <td>-0.0796</td>\n",
       "      <td>-77.46</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.3708</td>\n",
       "      <td>95.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20191201</td>\n",
       "      <td>-3.1742</td>\n",
       "      <td>-66.22</td>\n",
       "      <td>14.98</td>\n",
       "      <td>43.5596</td>\n",
       "      <td>25.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20200101</td>\n",
       "      <td>0.1245</td>\n",
       "      <td>0.93</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0.4085</td>\n",
       "      <td>95.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20200211</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>-1.28</td>\n",
       "      <td>6.28</td>\n",
       "      <td>2.0671</td>\n",
       "      <td>46.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20200311</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>97.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190121</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>-3.27</td>\n",
       "      <td>8.76</td>\n",
       "      <td>0.1501</td>\n",
       "      <td>97.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190211</td>\n",
       "      <td>0.0920</td>\n",
       "      <td>-52.18</td>\n",
       "      <td>41.47</td>\n",
       "      <td>11.0220</td>\n",
       "      <td>12.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190311</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-4.46</td>\n",
       "      <td>9.74</td>\n",
       "      <td>0.9599</td>\n",
       "      <td>71.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190411</td>\n",
       "      <td>-0.0062</td>\n",
       "      <td>-4.52</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.1366</td>\n",
       "      <td>92.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190511</td>\n",
       "      <td>-0.0018</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0992</td>\n",
       "      <td>94.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190611</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-20.12</td>\n",
       "      <td>19.25</td>\n",
       "      <td>1.6958</td>\n",
       "      <td>33.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190711</td>\n",
       "      <td>-0.0037</td>\n",
       "      <td>-16.68</td>\n",
       "      <td>19.00</td>\n",
       "      <td>0.4289</td>\n",
       "      <td>32.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190811</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>-8.18</td>\n",
       "      <td>11.86</td>\n",
       "      <td>2.0319</td>\n",
       "      <td>8.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190911</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>7.69</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>54.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20191011</td>\n",
       "      <td>-0.1051</td>\n",
       "      <td>-17.55</td>\n",
       "      <td>16.70</td>\n",
       "      <td>0.1093</td>\n",
       "      <td>87.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20191111</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>-7.79</td>\n",
       "      <td>7.74</td>\n",
       "      <td>0.3552</td>\n",
       "      <td>95.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20191201</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>-2.84</td>\n",
       "      <td>5.94</td>\n",
       "      <td>6.8616</td>\n",
       "      <td>88.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20200101</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>97.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20200211</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>6.05</td>\n",
       "      <td>2.0563</td>\n",
       "      <td>46.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20200311</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>97.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           gf         d     err   errP  MdAPE      MSE  R2score\n",
       "0   withoutGF  20190121  0.0012  -3.57   8.76   0.1512    97.84\n",
       "1   withoutGF  20190211  0.2876 -62.45  45.28  10.9013    14.78\n",
       "2   withoutGF  20190311 -0.0018  -4.19   9.90   0.9537    71.77\n",
       "3   withoutGF  20190411  0.0577  -3.37  11.20   0.2112    88.81\n",
       "4   withoutGF  20190511 -0.0063  -3.24   8.26   0.1005    94.88\n",
       "5   withoutGF  20190611  0.0079 -19.19  19.14   1.6930    33.80\n",
       "6   withoutGF  20190711 -0.0088 -18.09  19.40   0.4316    31.98\n",
       "7   withoutGF  20190811 -0.0265 -14.74  14.37   2.0530     7.79\n",
       "8   withoutGF  20190911 -0.0008  -2.83   7.74   0.0258    54.43\n",
       "9   withoutGF  20191011 -0.1051 -17.55  16.70   0.1093    87.65\n",
       "10  withoutGF  20191111 -0.0796 -77.46  11.61   0.3708    95.41\n",
       "11  withoutGF  20191201 -3.1742 -66.22  14.98  43.5596    25.39\n",
       "12  withoutGF  20200101  0.1245   0.93   7.60   0.4085    95.97\n",
       "13  withoutGF  20200211  0.0215  -1.28   6.28   2.0671    46.30\n",
       "14  withoutGF  20200311  0.0077  -0.90   4.38   0.2584    97.42\n",
       "15     withGF  20190121  0.0056  -3.27   8.76   0.1501    97.86\n",
       "16     withGF  20190211  0.0920 -52.18  41.47  11.0220    12.57\n",
       "17     withGF  20190311 -0.0016  -4.46   9.74   0.9599    71.59\n",
       "18     withGF  20190411 -0.0062  -4.52   9.59   0.1366    92.76\n",
       "19     withGF  20190511 -0.0018  -3.00   8.22   0.0992    94.95\n",
       "20     withGF  20190611 -0.0016 -20.12  19.25   1.6958    33.69\n",
       "21     withGF  20190711 -0.0037 -16.68  19.00   0.4289    32.42\n",
       "22     withGF  20190811  0.0107  -8.18  11.86   2.0319     8.56\n",
       "23     withGF  20190911  0.0022  -1.70   7.69   0.0257    54.70\n",
       "24     withGF  20191011 -0.1051 -17.55  16.70   0.1093    87.65\n",
       "25     withGF  20191111  0.0031  -7.79   7.74   0.3552    95.60\n",
       "26     withGF  20191201  0.0386  -2.84   5.94   6.8616    88.25\n",
       "27     withGF  20200101  0.0177  -2.19   7.50   0.2790    97.24\n",
       "28     withGF  20200211  0.0119  -1.52   6.05   2.0563    46.58\n",
       "29     withGF  20200311  0.0077  -0.90   4.38   0.2584    97.42"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\slope'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "\n",
    "gf = []\n",
    "d = []\n",
    "err = []\n",
    "errP = []\n",
    "MdAPE = []\n",
    "MSE = []\n",
    "R2score = []\n",
    "\n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if ('tiff' in fileName) and ('actual' in fileName) ]\n",
    "    \n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        img = rio.open(os.path.join(subdir1,'actual_Tur_20190121.tiff'))        \n",
    "        file1 = files_temp[n]        \n",
    "        actual = rio.open(os.path.join(subdir1,file1)).read(1)\n",
    "        predicted = rio.open(os.path.join(subdir1,'predicted_'+file1[7:])).read(1)\n",
    "\n",
    "        actualValues = []\n",
    "        predValues = []\n",
    "        for i in range(actual.shape[0]):  # Exclude none values from this analysis\n",
    "            for j in range(actual.shape[1]):\n",
    "                if actual[i,j]<10000 and actual[i,j]>-90 and predicted[i,j]<10000 and predicted[i,j]>-90:\n",
    "                    actualValues.append(actual[i,j])\n",
    "                    predValues.append(predicted[i,j])\n",
    "        # Use error metrics that do not penalize large differences between actual and predicted\n",
    "        errorMetrics = evaluate(np.array(actualValues,dtype=np.float64), np.array(predValues,dtype=np.float64), metrics=('error', 'percentage_error','mdape','mse','R2_score'))\n",
    "    \n",
    "        gf.append(gf_folder)\n",
    "        d.append(file1[11:19])\n",
    "        err.append(round(errorMetrics['error'],4))\n",
    "        errP.append(round(100*errorMetrics['percentage_error'],2))\n",
    "        MdAPE.append(round(100*errorMetrics['mdape'],2))\n",
    "        MSE.append(round(errorMetrics['mse'],4))\n",
    "        R2score.append(round(100*errorMetrics['R2_score'],2))\n",
    "df = pd.DataFrame({'gf':gf,'d':d,'err':err, 'errP':errP, 'MdAPE':MdAPE,'MSE':MSE,'R2score':R2score})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T16:25:38.128867Z",
     "start_time": "2022-01-24T16:24:51.118881Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/15 [00:00<?, ?it/s]<ipython-input-4-54063a72afae>:19: RuntimeWarning: overflow encountered in multiply\n",
      "  arrayBias = 100*(actual-predicted)/(actual+ EPSILON)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:25<00:00,  1.68s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:21<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate over/under estimation maps (for areas that have been gap filled display None)\n",
    "# in NTU \n",
    "# Need to exclude pixels where turbidity is none while S2 is known\n",
    "\n",
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\slope'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "\n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if ('tiff' in fileName) and ('actual' in fileName) ]\n",
    "    \n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        img = rio.open(os.path.join(subdir1,'actual_Tur_20190121.tiff'))        \n",
    "        file1 = files_temp[n]        \n",
    "        actual = rio.open(os.path.join(subdir1,file1)).read(1)\n",
    "        predicted = rio.open(os.path.join(subdir1,'predicted_'+file1[7:])).read(1)\n",
    "        \n",
    "        arrayBias = 100*(actual-predicted)/(actual+ EPSILON)\n",
    "        for i in range(actual.shape[0]):  # Exclude none values from this analysis\n",
    "            for j in range(actual.shape[1]):\n",
    "                if actual[i,j] > 1e+36 or predicted[i,j] > 1e+36:\n",
    "                    arrayBias[i,j] = actual[i,j]\n",
    "        \n",
    "        # Export as image\n",
    "        outputdir1 = os.path.join(maindir2,gf_folder,'France','biasArray'+file1[7:19]+'.tiff')\n",
    "        with rio.open(outputdir1,'w',driver='Gtiff', width=img.width, height=img.height, \n",
    "                            count=1,crs=img.crs,transform=img.transform, dtype='float32', nodata=9.96921e+36) as newImg:\n",
    "            newImg.write(arrayBias,1)\n",
    "            newImg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
