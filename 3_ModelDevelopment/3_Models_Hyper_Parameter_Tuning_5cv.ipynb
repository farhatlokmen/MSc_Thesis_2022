{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T09:22:31.454697Z",
     "start_time": "2021-09-14T09:20:49.017920Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(4)\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam, Adam\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.constraints import maxnorm\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, KFold, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T09:22:31.500523Z",
     "start_time": "2021-09-14T09:22:31.495398Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_random_seed(x):\n",
    "    tf.random.set_seed(x) # Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    np.random.seed(x)     # Set the `numpy` pseudo-random generator at a fixed value\n",
    "    random.seed(x)        # Set the `python` built-in pseudo-random generator at a fixed value   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T09:22:31.617612Z",
     "start_time": "2021-09-14T09:22:31.538065Z"
    }
   },
   "outputs": [],
   "source": [
    "def coeff_determination(y_true, y_pred):\n",
    "        SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "        SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "        return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "my_scorer = make_scorer(r2_score)\n",
    "\n",
    "###############################################################\n",
    "# Reproducibility is a Problem when using parallel processing #\n",
    "###############################################################\n",
    "n_jobs = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T11:35:58.430342Z",
     "start_time": "2021-09-12T11:35:58.376731Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(optimizer='adam'):    # Optimizers update the weight parameters to minimize the loss function\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(9, activation='sigmoid', input_shape=(Nfeatures,))) # Hidden layer with 9 nodes # input_shape: input layer (n° features)\n",
    "    model.add(Dense(4, activation='sigmoid')) # Hidden Layer with 4 nodes\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output Layer: n° output variables\n",
    "    # Compile model\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=[coeff_determination])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T14:19:27.658558Z",
     "start_time": "2021-09-12T12:30:54.774819Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 15/15 [54:47<00:00, 219.14s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 15/15 [53:42<00:00, 214.83s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\ANN'\n",
    "gf_folders = ['withoutGF','withGF']\n",
    "# set random seed\n",
    "seed = 4\n",
    "set_random_seed(seed) \n",
    "\n",
    "GF = []\n",
    "DATE = []\n",
    "Nb_S2_used = []\n",
    "optim = []\n",
    "r2 = []\n",
    "for gf_folder in gf_folders:    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]\n",
    "    \n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        for file2 in os.listdir(os.path.join(subdir1,file1)):\n",
    "            if 'Best_4' not in file2:\n",
    "                # step0: Read and split data\n",
    "                excel_file = pd.read_excel(os.path.join(subdir1,file1,file2))\n",
    "                y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                X = excel_file.values\n",
    "                # Data Normalization\n",
    "                y = MinMaxScaler().fit_transform(y)\n",
    "                X = MinMaxScaler().fit_transform(X)\n",
    "                train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                ############### Tune Optimizer ###############            \n",
    "                Nfeatures = train_X.shape[1]\n",
    "                # create model\n",
    "                model = KerasRegressor(build_fn=build_model, epochs=500, batch_size=100, verbose=0)\n",
    "                # define the grid search parameters\n",
    "                param_grid = {'optimizer':['SGD', 'Adam', 'Adamax', 'Nadam']}\n",
    "                grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring = my_scorer, n_jobs=n_jobs)\n",
    "                grid_result = grid.fit(train_X, train_y) # Shuffle False\n",
    "\n",
    "                # step6: Save results\n",
    "                GF.append(gf_folder)\n",
    "                DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                Nb_S2_used.append(int(file2[17:18]))\n",
    "                optim.append(grid_result.best_params_['optimizer'])  \n",
    "                r2.append(round(grid_result.best_score_*100,2))\n",
    "\n",
    "results = pd.DataFrame({'Date':DATE,\n",
    "                        'Gapfilling':GF,\n",
    "                        'Nb_S2_used':Nb_S2_used,\n",
    "                        'Optimizer':optim,\n",
    "                        'R2':r2\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T14:19:27.929860Z",
     "start_time": "2021-09-12T14:19:27.912463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adam'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['Optimizer'].value_counts().idxmax() # Most frequent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T09:18:03.791638Z",
     "start_time": "2021-09-13T09:18:03.729937Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(learn_rate=0.01):    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(9, activation='sigmoid', input_shape=(Nfeatures,))) # Hidden layer with 9 nodes # input_shape: input layer (n° features)\n",
    "    model.add(Dense(4, activation='sigmoid')) # Hidden Layer with 4 nodes\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output Layer: n° output variables\n",
    "    # Compile model\n",
    "    optimizer = Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adam\")\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=[coeff_determination])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:47:19.584746Z",
     "start_time": "2021-09-13T09:18:03.863599Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 15/15 [1:14:44<00:00, 298.97s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 15/15 [1:14:27<00:00, 297.87s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\ANN'\n",
    "gf_folders = ['withoutGF','withGF']\n",
    "# set random seed\n",
    "seed = 4\n",
    "set_random_seed(seed) \n",
    "\n",
    "GF = []\n",
    "DATE = []\n",
    "Nb_S2_used = []\n",
    "lr = []\n",
    "r2 = []\n",
    "for gf_folder in gf_folders:    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]\n",
    "    \n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        for file2 in os.listdir(os.path.join(subdir1,file1)):\n",
    "            if 'Best_4' not in file2:\n",
    "                # step0: Read and split data\n",
    "                excel_file = pd.read_excel(os.path.join(subdir1,file1,file2))\n",
    "                y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                X = excel_file.values\n",
    "                # Data Normalization\n",
    "                y = MinMaxScaler().fit_transform(y)\n",
    "                X = MinMaxScaler().fit_transform(X)\n",
    "                train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                ############### Learning Rate & Momentum ###############\n",
    "                # lr: # how much to update the weight at the end of each batch\n",
    "                # beta_1 and beta_2: # adjust momentum # how much to let the previous update influence the current weight update\n",
    "                # epsilon: stabilize learning process\n",
    "\n",
    "                Nfeatures = train_X.shape[1]\n",
    "                # create model\n",
    "                model = KerasRegressor(build_fn=build_model, epochs=500, batch_size=100, verbose=0)\n",
    "                # define the grid search parameters\n",
    "                param_grid = {'learn_rate':[0.001, 0.01, 0.1, 0.2, 0.3]}\n",
    "                grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring = my_scorer, n_jobs=n_jobs)\n",
    "                grid_result = grid.fit(train_X, train_y)  \n",
    "\n",
    "                # step6: Save results\n",
    "                GF.append(gf_folder)\n",
    "                DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                Nb_S2_used.append(int(file2[17:18]))\n",
    "                lr.append(grid_result.best_params_['learn_rate'])  \n",
    "                r2.append(round(grid_result.best_score_*100,2))\n",
    "    \n",
    "results = pd.DataFrame({'Date':DATE,\n",
    "                        'Gapfilling':GF,\n",
    "                        'Nb_S2_used':Nb_S2_used,\n",
    "                        'learn_rate':lr,\n",
    "                        'R2':r2\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:47:20.219335Z",
     "start_time": "2021-09-13T11:47:20.211348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['learn_rate'].value_counts().idxmax() # Most frequent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:49:33.524390Z",
     "start_time": "2021-09-13T11:49:33.517406Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(learn_rate=0.01):    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(9, activation='sigmoid', input_shape=(Nfeatures,))) # Hidden layer with 9 nodes # input_shape: input layer (n° features)\n",
    "    model.add(Dense(4, activation='sigmoid')) # Hidden Layer with 4 nodes\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output Layer: n° output variables\n",
    "    # Compile model\n",
    "    optimizer = Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adam\")\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=[coeff_determination])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:00:50.085485Z",
     "start_time": "2021-09-13T11:49:34.174381Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 15/15 [1:34:58<00:00, 379.93s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 15/15 [1:36:16<00:00, 385.13s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\ANN'\n",
    "gf_folders = ['withoutGF','withGF']\n",
    "# set random seed\n",
    "seed = 4\n",
    "set_random_seed(seed) \n",
    "\n",
    "GF = []\n",
    "DATE = []\n",
    "Nb_S2_used = []\n",
    "batchSize = []\n",
    "epochsN = []\n",
    "r2 = []\n",
    "for gf_folder in gf_folders:    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]\n",
    "    \n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        for file2 in os.listdir(os.path.join(subdir1,file1)):\n",
    "            if 'Best_4' not in file2:\n",
    "                # step0: Read and split data\n",
    "                excel_file = pd.read_excel(os.path.join(subdir1,file1,file2))\n",
    "                y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                X = excel_file.values\n",
    "                # Data Normalization\n",
    "                y = MinMaxScaler().fit_transform(y)\n",
    "                X = MinMaxScaler().fit_transform(X)\n",
    "                train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                ############### Tune Batch Size & Number of Epochs ###############\n",
    "                # Batch size: n° of samples shown to the network before the weights are updated \n",
    "                # Epochs: n° of times that the entire training dataset is shown to the network during training\n",
    "                Nfeatures = train_X.shape[1]\n",
    "                # create model\n",
    "                model = KerasRegressor(build_fn=build_model, verbose=0)\n",
    "                # define the grid search parameters\n",
    "                param_grid = {'batch_size':[10, 20, 40, 60, 80, 100],\n",
    "                              'epochs':[10, 50, 100]}\n",
    "                grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring = my_scorer, n_jobs=n_jobs)\n",
    "                grid_result = grid.fit(train_X, train_y)\n",
    "\n",
    "                # step6: Save results\n",
    "                GF.append(gf_folder)\n",
    "                DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                Nb_S2_used.append(int(file2[17:18]))\n",
    "                batchSize.append(grid_result.best_params_['batch_size'])  \n",
    "                epochsN.append(grid_result.best_params_['epochs'])\n",
    "                r2.append(round(grid_result.best_score_*100,2))\n",
    "    \n",
    "results = pd.DataFrame({'Date':DATE,\n",
    "                        'Gapfilling':GF,\n",
    "                        'Nb_S2_used':Nb_S2_used,\n",
    "                        'batch_size':batchSize,\n",
    "                        'epochs':epochsN,\n",
    "                        'R2':r2\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:00:50.618983Z",
     "start_time": "2021-09-13T15:00:50.601502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['epochs'].value_counts().idxmax() # Most frequent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T15:00:51.110565Z",
     "start_time": "2021-09-13T15:00:51.101953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['batch_size'].value_counts().idxmax() # Most frequent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T09:22:31.744555Z",
     "start_time": "2021-09-14T09:22:31.645821Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(units1=1,units2=1):    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units1, activation='sigmoid', input_shape=(Nfeatures,))) # Hidden layer with 9 nodes # input_shape: input layer (n° features)\n",
    "    model.add(Dense(units2, activation='sigmoid')) # Hidden Layer with 4 nodes\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output Layer: n° output variables\n",
    "    # Compile model\n",
    "    optimizer = Adam(lr=0.01)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=[coeff_determination])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-14T09:21:03.751Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████▍                                             | 6/15 [8:06:07<12:09:10, 4861.19s/it]"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\ANN'\n",
    "gf_folders = ['withoutGF','withGF']\n",
    "# set random seed\n",
    "seed = 4\n",
    "set_random_seed(seed) \n",
    "\n",
    "GF = []\n",
    "DATE = []\n",
    "Nb_S2_used = []\n",
    "units1 = []\n",
    "units2 = []\n",
    "r2 = []\n",
    "for gf_folder in gf_folders:    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]\n",
    "    \n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        for file2 in os.listdir(os.path.join(subdir1,file1)):\n",
    "            if 'Best_4' not in file2:\n",
    "                # step0: Read and split data\n",
    "                excel_file = pd.read_excel(os.path.join(subdir1,file1,file2))\n",
    "                y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                X = excel_file.values\n",
    "                # Data Normalization\n",
    "                y = MinMaxScaler().fit_transform(y)\n",
    "                X = MinMaxScaler().fit_transform(X)\n",
    "                train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                ############### Tune the Network Design ###############\n",
    "                # N° Layers: fixed as 2\n",
    "                # N° Neurons: a large enough single layer network can approximate any other neural network, at least in theory.\n",
    "\n",
    "                Nfeatures = train_X.shape[1]\n",
    "                # create model\n",
    "                model = KerasRegressor(build_fn=build_model, epochs=100, batch_size=10, verbose=0)\n",
    "                # define the grid search parameters\n",
    "                param_grid = {'units1':[i for i in range(2,15,2)],\n",
    "                              'units2':[i for i in range(2,15,2)]}\n",
    "                grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring = my_scorer, n_jobs=n_jobs)\n",
    "                grid_result = grid.fit(train_X, train_y)\n",
    "\n",
    "                # step6: Save results\n",
    "                GF.append(gf_folder)\n",
    "                DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                Nb_S2_used.append(int(file2[17:18]))\n",
    "                units1.append(grid_result.best_params_['units1'])  \n",
    "                units2.append(grid_result.best_params_['units2'])\n",
    "                r2.append(round(grid_result.best_score_*100,2))\n",
    "    \n",
    "results = pd.DataFrame({'Date':DATE,\n",
    "                        'Gapfilling':GF,\n",
    "                        'Nb_S2_used':Nb_S2_used,\n",
    "                        'units1':units1,\n",
    "                        'units2':units2,\n",
    "                        'R2':r2\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-14T09:21:04.334Z"
    }
   },
   "outputs": [],
   "source": [
    "results['units1'].value_counts().idxmax() # Most frequent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-14T09:21:04.955Z"
    }
   },
   "outputs": [],
   "source": [
    "results['units2'].value_counts().idxmax() # Most frequent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T00:29:41.432529Z",
     "start_time": "2021-09-12T13:00:43.047Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(units1=14,units2=12,init_mode1='uniform', init_mode2='uniform' , init_mode3='uniform', activ_func1='sigmoid', activ_func2='sigmoid', activ_func3='sigmoid'):    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units1, kernel_initializer=init_mode1, activation=activ_func1, input_shape=(Nfeatures,))) # Hidden layer with 9 nodes # input_shape: input layer (n° features)\n",
    "    model.add(Dense(units2, kernel_initializer=init_mode2, activation=activ_func2)) # Hidden Layer with 4 nodes\n",
    "    model.add(Dense(1, kernel_initializer=init_mode3, activation=activ_func3)) # Output Layer: n° output variables\n",
    "    # Compile model\n",
    "    optimizer = Adam(lr=0.01)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=[coeff_determination])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T00:29:41.460271Z",
     "start_time": "2021-09-12T13:00:47.052Z"
    }
   },
   "outputs": [],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\ANN'\n",
    "gf_folders = ['withoutGF','withGF']\n",
    "# set random seed\n",
    "seed = 4\n",
    "set_random_seed(seed) \n",
    "\n",
    "GF = []\n",
    "DATE = []\n",
    "Nb_S2_used = []\n",
    "activ_func1 = []\n",
    "activ_func2 = []\n",
    "activ_func3 = []\n",
    "r2 = []\n",
    "for gf_folder in gf_folders:    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]\n",
    "    \n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        for file2 in os.listdir(os.path.join(subdir1,file1)):\n",
    "            if 'Best_4' not in file2:\n",
    "                # step0: Read and split data\n",
    "                excel_file = pd.read_excel(os.path.join(subdir1,file1,file2))\n",
    "                y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                X = excel_file.values\n",
    "                # Data Normalization\n",
    "                y = MinMaxScaler().fit_transform(y)\n",
    "                X = MinMaxScaler().fit_transform(X)\n",
    "                train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                ############### Tune Network Weight Initialization & Activation Function ###############\n",
    "                Nfeatures = train_X.shape[1]\n",
    "                # create model\n",
    "                model = KerasRegressor(build_fn=build_model, epochs=100, batch_size=10, verbose=0)\n",
    "                # define the grid search parameters\n",
    "                activ_func = ['softmax', 'relu', 'tanh', 'sigmoid']\n",
    "                param_grid = {'activ_func1':activ_func,\n",
    "                              'activ_func2':activ_func,\n",
    "                              'activ_func3':activ_func}\n",
    "                grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring = my_scorer, n_jobs=n_jobs)\n",
    "                grid_result = grid.fit(train_X, train_y)\n",
    "\n",
    "                # step6: Save results\n",
    "                GF.append(gf_folder)\n",
    "                DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                Nb_S2_used.append(int(file2[17:18]))\n",
    "                activ_func1.append(grid_result.best_params_['activ_func1'])\n",
    "                activ_func2.append(grid_result.best_params_['activ_func2'])\n",
    "                activ_func3.append(grid_result.best_params_['activ_func3'])\n",
    "                r2.append(round(grid_result.best_score_*100,2))\n",
    "\n",
    "results = pd.DataFrame({'Date':DATE,\n",
    "                        'Gapfilling':GF,\n",
    "                        'Nb_S2_used':Nb_S2_used,\n",
    "                        'activ_func1':activ_func1,\n",
    "                        'activ_func2':activ_func2,\n",
    "                        'activ_func3':activ_func3,\n",
    "                        'R2':r2\n",
    "                        })              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T00:29:41.477737Z",
     "start_time": "2021-09-12T13:00:48.402Z"
    }
   },
   "outputs": [],
   "source": [
    "results['activ_func1'].value_counts().idxmax() # Most frequent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T00:29:41.497936Z",
     "start_time": "2021-09-12T13:00:49.450Z"
    }
   },
   "outputs": [],
   "source": [
    "results['activ_func2'].value_counts().idxmax() # Most frequent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T00:29:41.525106Z",
     "start_time": "2021-09-12T13:00:50.357Z"
    }
   },
   "outputs": [],
   "source": [
    "results['activ_func3'].value_counts().idxmax() # Most frequent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = 'Adam'\n",
    "# learn_rate = 0.01\n",
    "# Epochs = 100\n",
    "# Batch_size = 10\n",
    "# Units1 = 14\n",
    "# Units2 = 12\n",
    "# ActivFunc1 = 'sigmoid'\n",
    "# ActivFunc2 = 'sigmoid'\n",
    "# ActivFunc3 = 'sigmoid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T16:14:48.623123Z",
     "start_time": "2022-01-15T16:14:47.839185Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, GridSearchCV, cross_validate\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T16:18:48.518256Z",
     "start_time": "2022-01-15T16:14:48.655006Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [03:59<00:00, 15.98s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\KNN'\n",
    "# gf_folders = ['withoutGF','withGF']\n",
    "gf_folders = ['withGF']\n",
    "seed = 4\n",
    "scoring = {'r2': 'r2',\n",
    "           'mae': 'neg_mean_absolute_error',\n",
    "           'rmse':'neg_root_mean_squared_error'}\n",
    "\n",
    "for gf_folder in gf_folders:    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]\n",
    "    GF = []\n",
    "    DATE = []\n",
    "    Nb_S2_used = []\n",
    "    n_neighbors = []\n",
    "    leaf_size = []\n",
    "    weights = []         \n",
    "    MAE = []            \n",
    "    RMSE = []  \n",
    "    r2 = []\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        for file2 in os.listdir(os.path.join(subdir1,file1)):\n",
    "            if 'Best_4' not in file2:\n",
    "                # step0: Read and split data\n",
    "                excel_file = pd.read_excel(os.path.join(subdir1,file1,file2))\n",
    "                y = excel_file.L0                                                                         # Target data\n",
    "                features = ['L'+str(i) for i in range(1,len(excel_file.columns)-3)]                       # Predictor variables\n",
    "                X = excel_file[features]                                                                  # Subset df            \n",
    "                train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                # step1: create random grid (construct a specified n° of combinations by randomly choosing possible parameter values)\n",
    "                random_grid = {'n_neighbors':[4,5,6,7,8],\n",
    "                              'leaf_size':[1,2,3,5],\n",
    "                              'weights':['uniform', 'distance']\n",
    "                              }                                                       \n",
    "\n",
    "                # step2: initiate the search for best combination\n",
    "                random_search = RandomizedSearchCV(estimator = KNeighborsRegressor(),     # Create base model\n",
    "                                                   param_distributions = random_grid,      # random selection of parameters\n",
    "                                                   n_iter = 40,                           # across all different combinations\n",
    "                                                   cv = 5,                                 # using 5 fold cross validation\n",
    "                                                   verbose=0,                              # Set it to see more information about the tree building process\n",
    "                                                   random_state=seed,                      # Note: A Random Forest uses randomised decision trees, and as such, each time you fit, the result will change\n",
    "                                                   n_jobs = -1)                            # use all available cores\n",
    "                random_search.fit(train_X, train_y)                                        # Fit the random search model\n",
    "\n",
    "                # step3: Calculate model using best parameters\n",
    "                kfold_indexes = list(KFold(10,shuffle=True,random_state=seed).split(train_X)) # split training into Kfolds and shuffle            \n",
    "                model = KNeighborsRegressor(n_neighbors=random_search.best_params_['n_neighbors'],\n",
    "                                            leaf_size=random_search.best_params_['leaf_size'],\n",
    "                                            weights=random_search.best_params_['weights']\n",
    "                                           )                                        # Define model parameters\n",
    "\n",
    "                # step4: Get scores of KFolds CV for verification\n",
    "                scores = cross_validate(model,train_X,train_y,cv=kfold_indexes,scoring=scoring,return_estimator=True)            \n",
    "                df = pd.DataFrame({'test_r2':scores['test_r2'],\n",
    "                                  'test_mae':scores['test_mae'],\n",
    "                                  'test_rmse':scores['test_rmse']})\n",
    "                \n",
    "                path1 = os.path.join(maindir2,'France',gf_folder,'initialTesting')\n",
    "                os.makedirs(path1, exist_ok=True)\n",
    "                outputdir1 = os.path.join(path1,'scores10Folds_'+file2[17:])\n",
    "                df.to_excel(outputdir1, encoding='utf-8', index=False)   \n",
    "\n",
    "                # step5: Get scores using all data\n",
    "                model.fit(train_X, train_y)                                          # Fit model based on training data\n",
    "                predictions = model.predict(val_X)                                         # Apply model on validation data            \n",
    "                MAE1 = mean_absolute_error(val_y, predictions)                      # Measure MAE (less sensitive to outliers compared to RMSE)\n",
    "                RMSE1 = mean_squared_error(val_y, predictions, squared=False)        # Measure RMSE (average error performed by the model in predicting the outcome for an observation)\n",
    "                Rsquared1 = r2_score(val_y, predictions)                            # Measure accuracy (coefficient of determination R^2: the proportion of variation in the outcome that is explained by the predictor variables)\n",
    "\n",
    "                # step6: Save results\n",
    "                GF.append(gf_folder)\n",
    "                DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                Nb_S2_used.append(int(file2[17:18]))\n",
    "                n_neighbors.append(random_search.best_params_['n_neighbors'])\n",
    "                leaf_size.append(random_search.best_params_['leaf_size'])\n",
    "                weights.append(random_search.best_params_['weights'])\n",
    "                MAE.append(round(MAE1,2))            \n",
    "                RMSE.append(round(RMSE1,2))  \n",
    "                r2.append(round(Rsquared1*100,2))\n",
    "            \n",
    "    results = pd.DataFrame({'Date':DATE,\n",
    "                            'Gapfilling':GF,\n",
    "                            'Nb_S2_used':Nb_S2_used,\n",
    "                            'n_neighbors':n_neighbors,\n",
    "                            'leaf_size':leaf_size,\n",
    "                            'weights':weights,\n",
    "                            'MAE':MAE,\n",
    "                            'RMSE':RMSE,\n",
    "                            'R2':r2\n",
    "                            })\n",
    "\n",
    "    # step7: Export as excel files\n",
    "    path2 = os.path.join(maindir2,'France',gf_folder,'initialTesting')\n",
    "    os.makedirs(path2, exist_ok=True)\n",
    "    outputdir2 = os.path.join(path2,'Performance_'+gf_folder+'.xlsx')\n",
    "    results.to_excel(outputdir2, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T03:15:17.843404Z",
     "start_time": "2022-01-16T03:15:17.807915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 6, 5: 3, 7: 3, 6: 2, 8: 1})\n",
      "Counter({1: 15})\n",
      "Counter({'distance': 15})\n"
     ]
    }
   ],
   "source": [
    "maindir = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\KNN\\France'\n",
    "folder = 'withGF'   # withGF or withoutGF\n",
    "Nb_S2_used = 3       # 1, 2, 3, 4\n",
    "excel_file1 = pd.read_excel(os.path.join(maindir,folder,'initialTesting','Performance_'+folder+'.xlsx'))\n",
    "df = pd.DataFrame(excel_file1)\n",
    "df1 = df[df[\"Nb_S2_used\"]==Nb_S2_used]\n",
    "lst1 = list(df1[\"n_neighbors\"])\n",
    "lst2 = list(df1[\"leaf_size\"])\n",
    "lst3 = list(df1[\"weights\"])\n",
    "print(collections.Counter(lst1))\n",
    "print(collections.Counter(lst2))\n",
    "print(collections.Counter(lst3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## WithoutGF ########################\n",
    "# HyperParameters (most frequent)\n",
    "##### 1\n",
    "n_neighbors = 8\n",
    "leaf_size = 1\n",
    "weights = 'distance'\n",
    "##### 2\n",
    "n_neighbors = 8\n",
    "leaf_size = 1\n",
    "weights = 'distance'\n",
    "##### 3\n",
    "n_neighbors = 6\n",
    "leaf_size = 1\n",
    "weights = 'distance'\n",
    "######################## WithGF ########################\n",
    "# HyperParameters (most frequent)\n",
    "##### 1\n",
    "n_neighbors = 8\n",
    "leaf_size = 1\n",
    "weights = 'distance'\n",
    "##### 2\n",
    "n_neighbors = 8\n",
    "leaf_size = 1\n",
    "weights = 'distance'\n",
    "##### 3\n",
    "n_neighbors = 4\n",
    "leaf_size = 1\n",
    "weights = 'distance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Selected config (both withGF or withoutGF) ################\n",
    "\n",
    "n_neighbors = 8\n",
    "leaf_size = 1\n",
    "weights = 'distance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T16:18:49.477850Z",
     "start_time": "2022-01-15T16:18:49.447931Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, GridSearchCV, cross_validate\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T21:08:44.005482Z",
     "start_time": "2022-01-15T16:18:49.935237Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   26.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   24.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.1min finished\n",
      "  7%|█████▎                                                                         | 1/15 [20:59<4:53:56, 1259.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.6min finished\n",
      " 13%|██████████▌                                                                    | 2/15 [39:08<4:21:49, 1208.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.6min finished\n",
      " 20%|███████████████▊                                                               | 3/15 [58:19<3:58:13, 1191.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.4min finished\n",
      " 27%|████████████████████▌                                                        | 4/15 [1:16:14<3:32:00, 1156.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.6min finished\n",
      " 33%|█████████████████████████▋                                                   | 5/15 [1:36:22<3:15:19, 1171.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   27.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  8.4min finished\n",
      " 40%|██████████████████████████████▊                                              | 6/15 [1:58:07<3:01:44, 1211.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   25.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.8min finished\n",
      " 47%|███████████████████████████████████▉                                         | 7/15 [2:18:42<2:42:29, 1218.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   26.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  8.2min finished\n",
      " 53%|█████████████████████████████████████████                                    | 8/15 [2:41:37<2:27:39, 1265.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   25.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.9min finished\n",
      " 60%|██████████████████████████████████████████████▏                              | 9/15 [3:01:35<2:04:32, 1245.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   23.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.8min finished\n",
      " 67%|██████████████████████████████████████████████████▋                         | 10/15 [3:21:37<1:42:41, 1232.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  6.9min finished\n",
      " 73%|███████████████████████████████████████████████████████▋                    | 11/15 [3:38:12<1:17:24, 1161.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.2min finished\n",
      " 80%|██████████████████████████████████████████████████████████████▍               | 12/15 [3:56:13<56:51, 1137.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   24.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.1min finished\n",
      " 87%|███████████████████████████████████████████████████████████████████▌          | 13/15 [4:13:27<36:52, 1106.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.1min finished\n",
      " 93%|████████████████████████████████████████████████████████████████████████▊     | 14/15 [4:32:43<18:41, 1121.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.3min finished\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 15/15 [4:49:53<00:00, 1159.60s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\RF'\n",
    "# gf_folders = ['withoutGF','withGF']\n",
    "gf_folders = ['withGF']\n",
    "seed = 4\n",
    "scoring = {'r2': 'r2',\n",
    "           'mae': 'neg_mean_absolute_error',\n",
    "           'rmse':'neg_root_mean_squared_error'}\n",
    "\n",
    "for gf_folder in gf_folders:    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]\n",
    "    GF = []\n",
    "    DATE = []\n",
    "    Nb_S2_used = []\n",
    "    n_estimators = []\n",
    "    max_features = []\n",
    "    max_depth = []         \n",
    "    MAE = []            \n",
    "    RMSE = []  \n",
    "    r2 = []\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        for file2 in os.listdir(os.path.join(subdir1,file1)):\n",
    "            if 'Best_4' not in file2:\n",
    "                # step0: Read and split data\n",
    "                excel_file = pd.read_excel(os.path.join(subdir1,file1,file2))\n",
    "                y = excel_file.L0                                                                         # Target data\n",
    "                features = ['L'+str(i) for i in range(1,len(excel_file.columns)-3)]                       # Predictor variables\n",
    "                X = excel_file[features]                                                                  # Subset df            \n",
    "                train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                # step1: create random grid (construct a specified n° of combinations by randomly choosing possible parameter values)\n",
    "                random_grid = {'n_estimators': [int(x) for x in range(50,501,50)],            # Number of trees in random forest\n",
    "                               'max_features': [int(len(features) / 3.0)],                    # Number of features to consider at every split\n",
    "                               'max_depth': [int(x) for x in range(5,61,5)]                   # Maximum number of levels in tree\n",
    "                               }                                                       \n",
    "\n",
    "                # step2: initiate the search for best combination\n",
    "                random_search = RandomizedSearchCV(estimator = RFR(random_state=seed),     # Create base model\n",
    "                                                   param_distributions = random_grid,      # random selection of parameters\n",
    "                                                   n_iter = 100,                            # across 200 different combinations\n",
    "                                                   cv = 5,                                 # using 5 fold cross validation\n",
    "                                                   verbose=1,                              # Set it to see more information about the tree building process\n",
    "                                                   random_state=seed,                      # Note: A Random Forest uses randomised decision trees, and as such, each time you fit, the result will change\n",
    "                                                   n_jobs = -1)                            # use all available cores\n",
    "                random_search.fit(train_X, train_y)                                        # Fit the random search model\n",
    "\n",
    "                # step3: Calculate model using best parameters\n",
    "                kfold_indexes = list(KFold(10,shuffle=True,random_state=seed).split(train_X)) # split training into Kfolds and shuffle            \n",
    "                forestModel = RFR(n_estimators=random_search.best_params_['n_estimators'],\n",
    "                                  max_features=random_search.best_params_['max_features'],\n",
    "                                  max_depth=random_search.best_params_['max_depth'],\n",
    "                                  random_state=seed)                                        # Define model parameters\n",
    "                # step4: Get scores of KFolds CV for verification\n",
    "                scores = cross_validate(forestModel,train_X,train_y,cv=kfold_indexes,scoring=scoring,return_estimator=True)            \n",
    "                df = pd.DataFrame({'test_r2':scores['test_r2'],\n",
    "                                  'test_mae':scores['test_mae'],\n",
    "                                  'test_rmse':scores['test_rmse']})\n",
    "                \n",
    "                path1 = os.path.join(maindir2,'France',gf_folder,'initialTesting')\n",
    "                os.makedirs(path1, exist_ok=True)\n",
    "                outputdir1 = os.path.join(path1,'scores10Folds_'+file2[17:])\n",
    "                df.to_excel(outputdir1, encoding='utf-8', index=False)   \n",
    "\n",
    "                # step5: Get scores using all data\n",
    "                forestModel.fit(train_X, train_y)                                          # Fit model based on training data\n",
    "                predictions = forestModel.predict(val_X)                                         # Apply model on validation data            \n",
    "                MAE1 = mean_absolute_error(val_y, predictions)                      # Measure MAE (less sensitive to outliers compared to RMSE)\n",
    "                RMSE1 = mean_squared_error(val_y, predictions, squared=False)        # Measure RMSE (average error performed by the model in predicting the outcome for an observation)\n",
    "                Rsquared1 = r2_score(val_y, predictions)                            # Measure accuracy (coefficient of determination R^2: the proportion of variation in the outcome that is explained by the predictor variables)\n",
    "\n",
    "                # step6: Save results\n",
    "                GF.append(gf_folder)\n",
    "                DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                Nb_S2_used.append(int(file2[17:18]))\n",
    "                n_estimators.append(random_search.best_params_['n_estimators'])\n",
    "                max_features.append(random_search.best_params_['max_features'])\n",
    "                max_depth.append(random_search.best_params_['max_depth'])            \n",
    "                MAE.append(round(MAE1,2))            \n",
    "                RMSE.append(round(RMSE1,2))  \n",
    "                r2.append(round(Rsquared1*100,2))\n",
    "    \n",
    "    results = pd.DataFrame({'Date':DATE,\n",
    "                            'Gapfilling':GF,\n",
    "                            'Nb_S2_used':Nb_S2_used,\n",
    "                            'n_estimators':n_estimators,\n",
    "                            'max_features':max_features,\n",
    "                            'max_depth':max_depth,\n",
    "                            'MAE':MAE,\n",
    "                            'RMSE':RMSE,\n",
    "                            'R2':r2\n",
    "                            })\n",
    "\n",
    "    # step7: Export as excel files\n",
    "    path2 = os.path.join(maindir2,'France',gf_folder,'initialTesting')\n",
    "    os.makedirs(path2, exist_ok=True)                \n",
    "    outputdir2 = os.path.join(path2,'Performance_'+gf_folder+'.xlsx')\n",
    "    results.to_excel(outputdir2, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T03:17:51.012525Z",
     "start_time": "2022-01-16T03:17:50.977587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({500: 6, 100: 3, 200: 2, 50: 2, 300: 1, 450: 1})\n",
      "Counter({20: 5, 25: 4, 60: 1, 30: 1, 15: 1, 35: 1, 40: 1, 10: 1})\n"
     ]
    }
   ],
   "source": [
    "maindir = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\RF\\France'\n",
    "folder = 'withGF'   # withGF or withoutGF\n",
    "Nb_S2_used = 3         # 1, 2, 3, 4\n",
    "excel_file1 = pd.read_excel(os.path.join(maindir,folder,'initialTesting','Performance_'+folder+'.xlsx'))\n",
    "df = pd.DataFrame(excel_file1)\n",
    "df1 = df[df[\"Nb_S2_used\"]==Nb_S2_used]\n",
    "lst1 = list(df1[\"n_estimators\"])\n",
    "lst2 = list(df1[\"max_depth\"])\n",
    "print(collections.Counter(lst1))\n",
    "print(collections.Counter(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## WithoutGF ########################\n",
    "# HyperParameters (most frequent)\n",
    "##### 1\n",
    "n_estimators = 450\n",
    "max_depth = 10\n",
    "##### 2\n",
    "n_estimators = 50 or 150 or 500\n",
    "max_depth = 10\n",
    "##### 3\n",
    "n_estimators = 150\n",
    "max_depth = 15\n",
    "######################## WithGF ########################\n",
    "# HyperParameters (most frequent)\n",
    "##### 1\n",
    "n_estimators = 450\n",
    "max_depth = 10\n",
    "##### 2\n",
    "n_estimators = 200 or 500\n",
    "max_depth = 25\n",
    "##### 3\n",
    "n_estimators = 500\n",
    "max_depth = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Selected config (both withGF or withoutGF) ################\n",
    "n_estimators = 500\n",
    "max_depth = 25\n",
    "max_features=int(len(features) / 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
