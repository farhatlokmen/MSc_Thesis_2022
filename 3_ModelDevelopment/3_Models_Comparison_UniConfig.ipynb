{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T04:24:25.362875Z",
     "start_time": "2022-01-16T04:24:21.280037Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(4)\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import rasterio as rio\n",
    "from copy import deepcopy\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam, Adam\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor as ANN\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNN\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.colors as colors\n",
    "from rasterio.plot import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T04:24:25.430130Z",
     "start_time": "2022-01-16T04:24:25.415111Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_random_seed(x):\n",
    "    tf.random.set_seed(x) # Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    np.random.seed(x)     # Set the `numpy` pseudo-random generator at a fixed value\n",
    "    random.seed(x)        # Set the `python` built-in pseudo-random generator at a fixed value      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T04:24:25.612869Z",
     "start_time": "2022-01-16T04:24:25.484062Z"
    }
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Reproducibility is a Problem when using parallel processing  (n_jobs = 1)#\n",
    "############################################################################ \n",
    "seed = 4\n",
    "set_random_seed(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T12:09:03.435942Z",
     "start_time": "2022-01-16T12:09:03.426961Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(learn_rate=0.01, units1=14,units2=12,activ_func1='sigmoid',activ_func2='sigmoid',activ_func3='sigmoid'):    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units1, kernel_initializer='uniform', activation=activ_func1, input_shape=(Nfeatures,))) \n",
    "    model.add(Dense(units2, kernel_initializer='uniform', activation=activ_func2))                           \n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation=activ_func3))\n",
    "    optimizer = Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adam\")\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T15:46:15.423771Z",
     "start_time": "2022-01-16T12:09:10.353468Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [01:33<00:00,  6.23s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 15/15 [56:39<00:00, 226.66s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 15/15 [2:38:48<00:00, 635.25s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning'\n",
    "ml_models = ['KNN','RF','ANN']\n",
    "# gf_folders = ['withoutGF','withGF']\n",
    "gf_folders = ['withGF']\n",
    "scoring = {'mse':'neg_mean_squared_error', 'r2': 'r2'}\n",
    "\n",
    "Model = []\n",
    "GF = []\n",
    "DATE = []\n",
    "Nb_S2_used = []\n",
    "mse = []\n",
    "r2 = []\n",
    "        \n",
    "for ml_model in ml_models:    \n",
    "    for gf_folder in gf_folders:    \n",
    "        subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "        files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]        \n",
    "        \n",
    "        for n in tqdm(range(len(files_temp))):\n",
    "            file1 = files_temp[n]\n",
    "            for file2 in os.listdir(os.path.join(subdir1,file1)): \n",
    "                if 'Best_4' not in file2:\n",
    "                    excel_file = pd.read_excel(os.path.join(subdir1,file1,file2)) # step0: Read and split data\n",
    "                    y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                    excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                    features = ['L'+str(i) for i in range(1,len(excel_file.columns)-3)]<\n",
    "                    X = excel_file.values                \n",
    "                    y = MinMaxScaler().fit_transform(y) # Data Normalization\n",
    "                    X = MinMaxScaler().fit_transform(X)\n",
    "                    train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                    kfold_indexes = list(KFold(10,shuffle=True,random_state=seed).split(train_X)) # split training into Kfolds and shuffle            \n",
    "                    Nfeatures = train_X.shape[1]\n",
    "                    ############### Model with selected hyper-parameters w/o cv (get scores using all data) (n_jobs=1 : to ensure replicability) ###############\n",
    "                    if ml_model == 'ANN':                    \n",
    "                        model = ANN(build_fn=build_model, epochs=100, batch_size=10, verbose=0)  # create model \n",
    "                    elif ml_model == 'KNN':                    \n",
    "                        train_y = train_y.ravel() # flatten to 1d array # data is in a column format while it expected it in a row.\n",
    "                        model = KNN(n_neighbors=8, leaf_size=1, weights='distance')\n",
    "                    elif ml_model == 'RF':\n",
    "                        train_y = train_y.ravel() # flatten to 1d array # data is in a column format while it expected it in a row.\n",
    "                        model = RFR(n_estimators=500, max_features=int(len(features)/3.0), max_depth=25, random_state=seed)\n",
    "\n",
    "                    scores = cross_validate(model,train_X,train_y,cv=kfold_indexes,scoring=scoring,return_estimator=True)            \n",
    "                    avg_mse = np.mean(scores['test_mse'])                \n",
    "                    avg_r2 = np.mean(scores['test_r2'])  \n",
    "                    ############### Save Results ###############\n",
    "                    Model.append(ml_model)\n",
    "                    GF.append(gf_folder)\n",
    "                    DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                    Nb_S2_used.append(int(file2[17:18]))\n",
    "                    mse.append(round(-avg_mse,4)) # the computed values are negative\n",
    "                    r2.append(round(avg_r2,4))    \n",
    "\n",
    "############### Export Results ###############                \n",
    "results = pd.DataFrame({'Model':Model,\n",
    "                        'Date':DATE,\n",
    "                        'Gapfilling':GF,\n",
    "                        'Nb_S2_used':Nb_S2_used,\n",
    "                        'mse':mse,\n",
    "                        'r2':r2\n",
    "                        })\n",
    "outputdir2 = os.path.join(maindir2,'10k_cvResults_all3Models.xlsx')\n",
    "results.to_excel(outputdir2, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select best learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T12:11:31.314335Z",
     "start_time": "2021-10-01T12:11:31.309350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Selected model: RF\n",
    "# N°S2: 2\n",
    "\n",
    "# run the algorithm on all training data\n",
    "# check MSE and R2 are similar to previous\n",
    "# Get feature importance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:47:21.191203Z",
     "start_time": "2022-01-17T04:44:34.265880Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [02:46<00:00, 11.10s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning'\n",
    "ml_models = ['RF']\n",
    "# gf_folders = ['withoutGF', 'withGF']\n",
    "gf_folders = ['withGF']\n",
    "\n",
    "Model = []\n",
    "GF = []\n",
    "DATE = []\n",
    "Nb_S2_used = []\n",
    "mse = []\n",
    "r2 = []\n",
    "best3Features = []\n",
    "        \n",
    "for ml_model in ml_models:    \n",
    "    for gf_folder in gf_folders:    \n",
    "        subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "        files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]        \n",
    "        \n",
    "        for n in tqdm(range(len(files_temp))):\n",
    "            file1 = files_temp[n]\n",
    "            for file2 in os.listdir(os.path.join(subdir1,file1)): \n",
    "                if 'Best_2' in file2:\n",
    "                    excel_file = pd.read_excel(os.path.join(subdir1,file1,file2)) # step0: Read and split data\n",
    "                    y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                    excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                    features = ['L'+str(i) for i in range(1,len(excel_file.columns)-3)]\n",
    "                    X = excel_file.values                \n",
    "                    y = MinMaxScaler().fit_transform(y) # Data Normalization\n",
    "                    X = MinMaxScaler().fit_transform(X)\n",
    "                    train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                    Nfeatures = train_X.shape[1]\n",
    "                    train_y = train_y.ravel() # flatten to 1d array # data is in a column format while it expected it in a row.\n",
    "                    \n",
    "                    ############### Predict turbidity using RF ()###############\n",
    "                    model = RFR(n_estimators=500, max_features=int(len(features)/3.0), max_depth=10, random_state=seed)                    \n",
    "                    model.fit(train_X, train_y)\n",
    "                    y_pred = model.predict(val_X)\n",
    "                    \n",
    "                    importance = model.feature_importances_\n",
    "                    indices = sorted(range(len(importance)), key=lambda i: importance[i])[-3:]\n",
    "                    dict_temp = {'B':[1,10,19,28],'G':[4,13,22,31],'R':[7,16,25,34],\n",
    "                                 'BG':[2,11,20,29],'GB':[5,14,23,32],'RB':[8,17,26,35],\n",
    "                                 'BR':[3,12,21,30],'GR':[6,15,24,33],'RG':[9,18,27,36]}\n",
    "                    best3F = ''\n",
    "                    for i in indices:\n",
    "                        for key,values in dict_temp.items():\n",
    "                            if i+1 in values: # Add 1 because the indices were counted from 0 whilst layer names start from L1\n",
    "                                best3F+=key+' '\n",
    "                    \n",
    "                    Model.append(ml_model)\n",
    "                    GF.append(gf_folder)\n",
    "                    DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                    Nb_S2_used.append(int(file2[17:18]))\n",
    "                    mse.append(round(mean_squared_error(val_y, y_pred, squared=True),4))\n",
    "                    r2.append(round(r2_score(val_y, y_pred),4)) \n",
    "                    best3Features.append(best3F)\n",
    "\n",
    "results = pd.DataFrame({'Model':Model,\n",
    "                        'Date':DATE,\n",
    "                        'Gapfilling':GF,\n",
    "                        'Nb_S2_used':Nb_S2_used,\n",
    "                        'mse':mse,\n",
    "                        'r2':r2,\n",
    "                        'best3Features':best3Features\n",
    "                        })\n",
    "\n",
    "# step7: Export as excel files\n",
    "outputdir2 = os.path.join(maindir2,'performanceResults_1Model.xlsx')\n",
    "results.to_excel(outputdir2, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T14:16:27.557130Z",
     "start_time": "2021-10-02T13:59:18.722682Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [10:16<00:00, 41.13s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [06:50<00:00, 27.39s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning'\n",
    "ml_models = ['RF']\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "\n",
    "Model = []\n",
    "GF = []\n",
    "DATE = []\n",
    "Nb_S2_used = []\n",
    "mse = []\n",
    "r2 = []\n",
    "        \n",
    "for ml_model in ml_models:    \n",
    "    for gf_folder in gf_folders:    \n",
    "        subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "        files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]        \n",
    "        \n",
    "        for n in tqdm(range(len(files_temp))):\n",
    "            file1 = files_temp[n]\n",
    "            for file2 in os.listdir(os.path.join(subdir1,file1)): \n",
    "                if 'Best_2' in file2:\n",
    "                    excel_file = pd.read_excel(os.path.join(subdir1,file1,file2)) # step0: Read and split data\n",
    "                    y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                    excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                    features = ['L'+str(i) for i in range(1,len(excel_file.columns)-3)]\n",
    "                    X = excel_file.values                \n",
    "                    y = MinMaxScaler().fit_transform(y) # Data Normalization\n",
    "                    X = MinMaxScaler().fit_transform(X)\n",
    "                    train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                    Nfeatures = train_X.shape[1]\n",
    "                    \n",
    "                    ############### Predict turbidity using ANN ()###############\n",
    "                    model = ANN(build_fn=build_model, epochs=100, batch_size=10, verbose=0)  # create model                    \n",
    "                    model.fit(train_X, train_y)\n",
    "                    y_pred = model.predict(val_X)\n",
    "                    \n",
    "                    Model.append(ml_model)\n",
    "                    GF.append(gf_folder)\n",
    "                    DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                    Nb_S2_used.append(int(file2[17:18]))\n",
    "                    mse.append(round(mean_squared_error(val_y, y_pred, squared=True),4))\n",
    "                    r2.append(round(r2_score(val_y, y_pred),4)) \n",
    "                    \n",
    "results = pd.DataFrame({'Model':Model,\n",
    "                        'Date':DATE,\n",
    "                        'Gapfilling':GF,\n",
    "                        'Nb_S2_used':Nb_S2_used,\n",
    "                        'mse':mse,\n",
    "                        'r2':r2\n",
    "                        })\n",
    "\n",
    "# step7: Export as excel files\n",
    "outputdir2 = os.path.join(maindir2,'perfANN.xlsx')\n",
    "results.to_excel(outputdir2, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T14:17:08.420840Z",
     "start_time": "2021-10-02T14:16:27.756629Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:20<00:00,  1.34s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:20<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning'\n",
    "ml_models = ['RF']\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "\n",
    "Model = []\n",
    "GF = []\n",
    "DATE = []\n",
    "Nb_S2_used = []\n",
    "mse = []\n",
    "r2 = []\n",
    "        \n",
    "for ml_model in ml_models:    \n",
    "    for gf_folder in gf_folders:    \n",
    "        subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "        files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]        \n",
    "        \n",
    "        for n in tqdm(range(len(files_temp))):\n",
    "            file1 = files_temp[n]\n",
    "            for file2 in os.listdir(os.path.join(subdir1,file1)): \n",
    "                if 'Best_2' in file2:\n",
    "                    excel_file = pd.read_excel(os.path.join(subdir1,file1,file2)) # step0: Read and split data\n",
    "                    y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                    excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                    features = ['L'+str(i) for i in range(1,len(excel_file.columns)-3)]\n",
    "                    X = excel_file.values                \n",
    "                    y = MinMaxScaler().fit_transform(y) # Data Normalization\n",
    "                    X = MinMaxScaler().fit_transform(X)\n",
    "                    train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                    Nfeatures = train_X.shape[1]\n",
    "                    train_y = train_y.ravel() # flatten to 1d array # data is in a column format while it expected it in a row.\n",
    "                    \n",
    "                    ############### Predict turbidity using KNN ()###############\n",
    "                    model = KNN(n_neighbors=8, leaf_size=1, weights='distance')\n",
    "                    model.fit(train_X, train_y)\n",
    "                    y_pred = model.predict(val_X)\n",
    "                    \n",
    "                    Model.append(ml_model)\n",
    "                    GF.append(gf_folder)\n",
    "                    DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                    Nb_S2_used.append(int(file2[17:18]))\n",
    "                    mse.append(round(mean_squared_error(val_y, y_pred, squared=True),4))\n",
    "                    r2.append(round(r2_score(val_y, y_pred),4)) \n",
    "                    \n",
    "results = pd.DataFrame({'Model':Model,\n",
    "                        'Date':DATE,\n",
    "                        'Gapfilling':GF,\n",
    "                        'Nb_S2_used':Nb_S2_used,\n",
    "                        'mse':mse,\n",
    "                        'r2':r2\n",
    "                        })\n",
    "\n",
    "# step7: Export as excel files\n",
    "outputdir2 = os.path.join(maindir2,'perfKNN.xlsx')\n",
    "results.to_excel(outputdir2, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new turbidity maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T13:16:47.445091Z",
     "start_time": "2022-01-14T13:16:47.433637Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPixelValue(array,idx1,idx2,idx3):\n",
    "    return array[idx1,idx2,idx3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T13:16:48.124Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 15/15 [38:27<00:00, 153.86s/it]\n",
      " 47%|██████████████████████████████████████▋                                            | 7/15 [08:02<09:15, 69.43s/it]"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\preparedInputData'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "\n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    subdir2 = os.path.join(maindir2,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' in fileName]\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]        \n",
    "        img = rio.open(os.path.join(subdir1,file1)) # start by reading all layers\n",
    "        arr = img.read()\n",
    "        # Rank S2 scenes based on n° KP\n",
    "        indices = [i for i in range(1,36,9)]\n",
    "        nb_KP = []\n",
    "        for i in indices:\n",
    "            temp_copy = deepcopy(arr[i])\n",
    "            temp_copy[temp_copy==-99]=9.96921e+36\n",
    "            nb_KP.append(len(np.argwhere(temp_copy<=1e+36).tolist()))        \n",
    "        df1 = pd.DataFrame({'indices':indices,'nb_KP':nb_KP})\n",
    "        df1.sort_values('nb_KP', inplace=True)  # order based on nb_KP and make changes to df permanent (order from worst to best)\n",
    "        df1.reset_index(drop=True, inplace=True) # Drop old index and make changes to df permanent\n",
    "        # Select reflectance layers associated with best 2 images\n",
    "        name = 'Pixels_From_Best_2_S2_'+file1[7:15]\n",
    "        l = list(df1[2:]['indices']) # out of the 4 indices, removes the 1st two\n",
    "        # Create a new stacked array of layers to be used\n",
    "        arr_temp = np.expand_dims(arr[0], axis=0)\n",
    "        for k1 in l:\n",
    "            for k2 in range(k1,k1+9):\n",
    "                arr_temp = np.append(arr_temp,np.expand_dims(arr[k2], axis=0),axis=0) # get 19 layers (1st layer is turbidity + 18 layers of best S2 images and associated combinations of bands )\n",
    "\n",
    "        # Get all possible pixel coordinates for valid or none pixel values (=9.96921e+36)\n",
    "        idX = []\n",
    "        idY = [] \n",
    "        idX_none = []\n",
    "        idY_none = []\n",
    "        for idx in range(arr_temp.shape[1]):    # get all pixel coordinates\n",
    "            for idy in range(arr_temp.shape[2]):\n",
    "                if arr_temp[1,idx,idy] <= 1e+36 or arr_temp[10,idx,idy] <= 1e+36:  # if both s2 images have known values\n",
    "                    if arr_temp[1,idx,idy] != -99 or arr_temp[10,idx,idy] != -99:\n",
    "                        idX.append(idx)                                      # 1: 1st best S2 image # 10: 2nd best S2 image \n",
    "                        idY.append(idy)\n",
    "                    else:\n",
    "                        idX_none.append(idx)                                            \n",
    "                        idY_none.append(idy)\n",
    "                else:\n",
    "                    idX_none.append(idx)                                            \n",
    "                    idY_none.append(idy)                    \n",
    "        \n",
    "        # Store all pixel values (!=none) in an empty df            \n",
    "        rows = ['L'+str(index) for index in range(len(arr_temp))]\n",
    "        columns = [index for index in range(len(idX))]\n",
    "        results = pd.DataFrame(index=rows, columns=columns)\n",
    "        data = [] # It is recommended to collect data in a list of lists and then assign it to a df (Than modifying a df each iteration => time costly and prone to error of dtypes)\n",
    "        for idxLayer in range(len(arr_temp)):\n",
    "            pixelValues = Parallel(n_jobs=-1)(delayed(getPixelValue)(arr_temp,idxLayer,idX[k],idY[k]) for k in range(len(idX)))\n",
    "            data.append(pixelValues)\n",
    "        results = pd.DataFrame(data, index=rows, columns=columns).T\n",
    "        results.insert(loc=0, column='idx', value=idX)   # Add coordinates to df (while specifying position)\n",
    "        results.insert(loc=1, column='idy', value=idY)  \n",
    "        # Store all pixel values (==none) in an empty df            \n",
    "        results_none = pd.DataFrame({'idx_none':idX_none, 'idy_none':idY_none})\n",
    "        \n",
    "        # Export as excel files\n",
    "        os.makedirs(subdir2, exist_ok=True)\n",
    "        outputdir = os.path.join(subdir2, name+'.xlsx')\n",
    "        results.to_excel(outputdir, encoding='utf-8')\n",
    "        \n",
    "        outputdir2 = os.path.join(subdir2,'coordsNonePixelValues'+str(n)+'.xlsx') \n",
    "        results_none.to_excel(outputdir2, encoding='utf-8', index=False) # The coords of none pixel values are the same # save them 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T13:16:49.026Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# generate new turbidity maps #\n",
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T13:16:49.794Z"
    }
   },
   "outputs": [],
   "source": [
    "# predict turbidity using all training dataset\n",
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\preparedInputData'\n",
    "maindir3 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "    \n",
    "for gf_folder in gf_folders:    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    subdir2 = os.path.join(maindir2,gf_folder,'France')\n",
    "    subdir3 = os.path.join(maindir3,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]        \n",
    "\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        for file2 in os.listdir(os.path.join(subdir1,file1)): \n",
    "            if 'Best_2' in file2:\n",
    "                ############### Read all training dataset (without splitting) ###############\n",
    "                # first train the model with the previously prepared training set. Then, apply the model to predict turbidity in whole study area #\n",
    "                excel_file = pd.read_excel(os.path.join(subdir1,file1,file2)) # step0: Read and split data\n",
    "                y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                features = ['L'+str(i) for i in range(1,len(excel_file.columns)-3)]\n",
    "                X = excel_file.values                \n",
    "#                 y = MinMaxScaler().fit_transform(y) # Data Normalization is not necessary for random forests\n",
    "#                 X = MinMaxScaler().fit_transform(X) # This will save us the time of invert normalization afterwards\n",
    "\n",
    "                Nfeatures = X.shape[1]\n",
    "                y = y.ravel() # flatten to 1d array # data is in a column format while it expected it in a row.\n",
    "                ############### Read all pixel values in 2 S2 images to predict corresponding turbidity values ###############\n",
    "                excel_file2 = pd.read_excel(os.path.join(subdir2,file2)) \n",
    "                idx = np.array(excel_file2['idx'].values,dtype=np.float).reshape(-1,1)                        \n",
    "                idy = np.array(excel_file2['idy'].values,dtype=np.float).reshape(-1,1)\n",
    "                excel_file2.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                features2 = ['L'+str(i) for i in range(1,len(excel_file2.columns)-3)]\n",
    "                S2_values = excel_file2.values                \n",
    "#                 S2_values = MinMaxScaler().fit_transform(S2_values) # Data Normalization\n",
    "                ############### Predict turbidity using RF ()###############\n",
    "                model = KNN(n_neighbors=8, leaf_size=1, weights='distance')\n",
    "                    \n",
    "        \n",
    "                # model = RFR(n_estimators=500, max_features=int(len(features)/3.0), max_depth=10, random_state=seed)                    \n",
    "                model.fit(X, y)\n",
    "                y_pred = model.predict(S2_values)\n",
    "                results = pd.DataFrame({'idx':idx.ravel(), 'idy':idy.ravel(), 'predTur':y_pred.ravel()})\n",
    "\n",
    "                # step7: Export as excel files\n",
    "                outputdir2 = os.path.join(subdir3,'Tur_'+file1+'.xlsx')\n",
    "                results.to_excel(outputdir2, encoding='utf-8', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T13:16:51.523Z"
    }
   },
   "outputs": [],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "    \n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    subdir2 = os.path.join(maindir2,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'None' in fileName]\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        # Read file: coordsNonePixelValues\n",
    "        excel_file1 = pd.read_excel(os.path.join(subdir1,file1)) # step0: Read and split data\n",
    "        idx_temp1 = list(excel_file1['idx_none'])\n",
    "        idy_temp1 = list(excel_file1['idy_none'])\n",
    "        noneValues = []\n",
    "        for i in range(excel_file1.shape[0]):\n",
    "            noneValues.append(9.96921e+36)\n",
    "        \n",
    "        df = pd.DataFrame({'idx':idx_temp1, 'idy':idy_temp1, 'predTur':noneValues})\n",
    "        # step7: Export as excel files\n",
    "        outputdir2 = os.path.join(subdir2,file1)\n",
    "        df.to_excel(outputdir2, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T13:16:59.999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read predicted turbidity pixel values and add the none values to it\n",
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "    \n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1, gf_folder, 'France')\n",
    "    subdir2 = os.path.join(maindir2, gf_folder, 'France')\n",
    "    files_temp1 = [fileName for fileName in os.listdir(subdir1) if ('Tur' in fileName)and('tiff' not in fileName)]  \n",
    "\n",
    "    for n in tqdm(range(len(files_temp1))):\n",
    "        file1 = files_temp1[n]\n",
    "        excel_file1 = pd.read_excel(os.path.join(subdir1,file1))\n",
    "        idx_temp1 = list(excel_file1['idx'])\n",
    "        idy_temp1 = list(excel_file1['idy'])\n",
    "        predTur_temp1 = list(excel_file1['predTur'])\n",
    "        \n",
    "        file2 = 'coordsNonePixelValues'+str(n)+'.xlsx'        \n",
    "        excel_file2 = pd.read_excel(os.path.join(subdir1,file2))        \n",
    "        idx_temp2 = list(excel_file2['idx'])\n",
    "        idy_temp2 = list(excel_file2['idy'])\n",
    "        predTur_temp2 = list(excel_file2['predTur'])\n",
    "        \n",
    "        idx = idx_temp1+idx_temp2\n",
    "        idy = idy_temp1+idy_temp2\n",
    "        predTur = predTur_temp1+predTur_temp2\n",
    "\n",
    "        results = pd.DataFrame({'idx':idx, 'idy':idy, 'predTur':predTur})\n",
    "        results.sort_values(by=['idx', 'idy'], ascending=True, inplace=True) # Sort Values by idx then by idy\n",
    "\n",
    "        rowsList = results['idx']\n",
    "        colList = results['idy']\n",
    "        turList = results['predTur']\n",
    "\n",
    "        file3 = file1[4:12]        \n",
    "        img = rio.open(os.path.join(subdir2,'merged_'+file3+'.tiff')) # start by reading all layers\n",
    "        arr = img.read()\n",
    "                \n",
    "        ######## Update Array ########  \n",
    "        # Export as images \n",
    "        temp_copy1 = deepcopy(arr[0]) # retain layer as actual turbidity\n",
    "        outputdir1 = os.path.join(subdir1, 'actual_'+file1[:-5]+'.tiff')\n",
    "        with rio.open(outputdir1,'w',driver='Gtiff', width=img.width, height=img.height, \n",
    "                            count=1,crs=img.crs,transform=img.transform, dtype='float32', nodata=9.96921e+36) as newImg:\n",
    "            newImg.write(temp_copy1,1)\n",
    "            newImg.close()\n",
    "        \n",
    "        temp_copy2 = deepcopy(arr[0]) # to be filled with predicted turbidity\n",
    "        for item in range(len(rowsList)):\n",
    "            temp_copy2[int(rowsList[item]),int(colList[item])] = turList[item]\n",
    "        outputdir2 = os.path.join(subdir1, 'predicted_'+file1[:-5]+'.tiff')\n",
    "        with rio.open(outputdir2,'w',driver='Gtiff', width=img.width, height=img.height, \n",
    "                            count=1,crs=img.crs,transform=img.transform, dtype='float32', nodata=9.96921e+36) as newImg:\n",
    "            newImg.write(temp_copy2,1)\n",
    "            newImg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T13:17:04.582Z"
    },
    "code_folding": [
     47,
     57
    ]
   },
   "outputs": [],
   "source": [
    "## https://gist.github.com/bshishov/5dc237f59f019b26145648e2124ca1c9\n",
    "\n",
    "EPSILON = 1e-10\n",
    "\n",
    "def _error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Simple error \"\"\"\n",
    "    return actual - predicted\n",
    "def _absolute_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" absolute error \"\"\"\n",
    "    return abs(actual - predicted)\n",
    "def _percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Percentage error\n",
    "    Note: result is NOT multiplied by 100\n",
    "    \"\"\"\n",
    "    return _error(actual, predicted) / (actual + EPSILON)\n",
    "def error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Simple error \"\"\"\n",
    "    return np.mean(_error(actual, predicted))\n",
    "def percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Percentage error\n",
    "    Note: result is NOT multiplied by 100\n",
    "    \"\"\"\n",
    "    return np.mean(_error(actual, predicted)/(actual + EPSILON))\n",
    "    \n",
    "def mse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Mean Squared Error \"\"\"\n",
    "    return np.mean(np.square(_error(actual, predicted)))\n",
    "def mdape(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Median Absolute Percentage Error\n",
    "    \"\"\"\n",
    "    return np.median(np.abs(_percentage_error(actual, predicted)))\n",
    "def R2_score(actual: np.ndarray, predicted: np.ndarray):\n",
    "    return r2_score(actual, predicted)\n",
    "\n",
    "METRICS = {\n",
    "    'mse': mse,\n",
    "    'mdape': mdape, # less affected by outliers\n",
    "    '_error':_error,\n",
    "    '_percentage_error':_percentage_error,\n",
    "    'error':error,\n",
    "    'percentage_error':percentage_error,\n",
    "    'R2_score':R2_score,\n",
    "}\n",
    "\n",
    "def evaluate(actual: np.ndarray, predicted: np.ndarray, metrics=('mse', 'mdape', '_error','_percentage_error', 'error','percentage_error','R2_score')):\n",
    "    results = {}\n",
    "    for name in metrics:\n",
    "        try:\n",
    "            results[name] = METRICS[name](actual, predicted)\n",
    "        except Exception as err:\n",
    "            results[name] = np.nan\n",
    "            print('Unable to compute metric {0}: {1}'.format(name, err))\n",
    "    return results\n",
    "\n",
    "def evaluate_all(actual: np.ndarray, predicted: np.ndarray):\n",
    "    return evaluate(actual, predicted, metrics=set(METRICS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T13:17:05.865Z"
    }
   },
   "outputs": [],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\slope'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "\n",
    "gf = []\n",
    "d = []\n",
    "err = []\n",
    "errP = []\n",
    "MdAPE = []\n",
    "MSE = []\n",
    "R2score = []\n",
    "\n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if ('tiff' in fileName) and ('actual' in fileName) ]\n",
    "    \n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        img = rio.open(os.path.join(subdir1,'actual_Tur_20190121.tiff'))        \n",
    "        file1 = files_temp[n]        \n",
    "        actual = rio.open(os.path.join(subdir1,file1)).read(1)\n",
    "        predicted = rio.open(os.path.join(subdir1,'predicted_'+file1[7:])).read(1)\n",
    "\n",
    "        actualValues = []\n",
    "        predValues = []\n",
    "        for i in range(actual.shape[0]):  # Exclude none values from this analysis\n",
    "            for j in range(actual.shape[1]):\n",
    "                if actual[i,j]<10000 and actual[i,j]>-90 and predicted[i,j]<10000 and predicted[i,j]>-90:\n",
    "                    actualValues.append(actual[i,j])\n",
    "                    predValues.append(predicted[i,j])\n",
    "        # Use error metrics that do not penalize large differences between actual and predicted\n",
    "        errorMetrics = evaluate(np.array(actualValues,dtype=np.float64), np.array(predValues,dtype=np.float64), metrics=('error', 'percentage_error','mdape','mse','R2_score'))\n",
    "    \n",
    "        gf.append(gf_folder)\n",
    "        d.append(file1[11:19])\n",
    "        err.append(round(errorMetrics['error'],4))\n",
    "        errP.append(round(100*errorMetrics['percentage_error'],2))\n",
    "        MdAPE.append(round(100*errorMetrics['mdape'],2))\n",
    "        MSE.append(round(errorMetrics['mse'],4))\n",
    "        R2score.append(round(100*errorMetrics['R2_score'],2))\n",
    "df = pd.DataFrame({'gf':gf,'d':d,'err':err, 'errP':errP, 'MdAPE':MdAPE,'MSE':MSE,'R2score':R2score})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
