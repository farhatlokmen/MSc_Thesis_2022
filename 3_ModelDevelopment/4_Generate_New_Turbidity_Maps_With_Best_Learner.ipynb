{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T16:24:27.079545Z",
     "start_time": "2022-01-24T16:24:16.607994Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(4)\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import rasterio as rio\n",
    "from copy import deepcopy\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam, Adam\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor as ANN\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNN\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.colors as colors\n",
    "from rasterio.plot import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-24T16:23:08.668Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_random_seed(x):\n",
    "    tf.random.set_seed(x) # Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    np.random.seed(x)     # Set the `numpy` pseudo-random generator at a fixed value\n",
    "    random.seed(x)        # Set the `python` built-in pseudo-random generator at a fixed value      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-24T16:23:09.703Z"
    }
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Reproducibility is a Problem when using parallel processing  (n_jobs = 1)#\n",
    "############################################################################ \n",
    "seed = 4\n",
    "set_random_seed(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T07:02:42.836497Z",
     "start_time": "2022-01-21T07:02:42.804351Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPixelValue(array,idx1,idx2,idx3):\n",
    "    return array[idx1,idx2,idx3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T07:22:23.798023Z",
     "start_time": "2022-01-21T07:03:10.883615Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [19:12<00:00, 76.85s/it]\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# Prepare excel files containing all pixel values of best 2 S2 (including missing values = -99)\n",
    "# Seperate None and valid pixels into 2 excel files\n",
    "##########\n",
    "# Without GF\n",
    "##########\n",
    "\n",
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\preparedInputData'\n",
    "gf_folders = ['withoutGF']\n",
    "\n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    subdir2 = os.path.join(maindir2,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' in fileName]\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]        \n",
    "        img = rio.open(os.path.join(subdir1,file1)) # start by reading all layers\n",
    "        arr = img.read()\n",
    "        # Rank S2 scenes based on n° KP \n",
    "        indices = [i for i in range(1,36,9)]\n",
    "        nb_KP = []\n",
    "        for i in indices:\n",
    "            temp_copy = deepcopy(arr[i])\n",
    "            temp_copy[temp_copy==-99]=9.96921e+36\n",
    "            nb_KP.append(len(np.argwhere(temp_copy<=1e+36).tolist()))        \n",
    "        df1 = pd.DataFrame({'indices':indices,'nb_KP':nb_KP})\n",
    "        df1.sort_values('nb_KP', inplace=True)  # order based on nb_KP and make changes to df permanent (order from worst to best)\n",
    "        df1.reset_index(drop=True, inplace=True) # Drop old index and make changes to df permanent\n",
    "        \n",
    "        # Select reflectance layers associated with best 2 images\n",
    "        name = 'Pixels_From_Best_2_S2_'+file1[7:15]\n",
    "        l = list(df1[2:]['indices']) # get the best 2 S2\n",
    "        # Create a new stacked array of layers to be used (from which we will extract coord of KP, UP, None pixels)\n",
    "        arr_temp = np.expand_dims(arr[0], axis=0)\n",
    "        for k1 in l:\n",
    "            for k2 in range(k1,k1+9):\n",
    "                arr_temp = np.append(arr_temp,np.expand_dims(arr[k2], axis=0),axis=0) # get 19 layers (1st layer is turbidity + 18 layers of best S2 images and associated combinations of bands )\n",
    "        \n",
    "        # Get all possible pixel coordinates for valid or none pixel values (=9.96921e+36)\n",
    "        idX = []\n",
    "        idY = [] \n",
    "        idX_none = []\n",
    "        idY_none = []\n",
    "        for idx in range(arr_temp.shape[1]):    # get all pixel coordinates\n",
    "            for idy in range(arr_temp.shape[2]):\n",
    "                if arr_temp[1,idx,idy] > 1e+36 or arr_temp[10,idx,idy] > 1e+36: # Exclude if pixel is none in one of the layers\n",
    "                    idX_none.append(idx)                                            \n",
    "                    idY_none.append(idy)\n",
    "                else:\n",
    "                    if arr_temp[1,idx,idy] ==-99 and arr_temp[10,idx,idy] ==-99: # Exclude if 2 S2 is UP\n",
    "                        idX_none.append(idx)                                            \n",
    "                        idY_none.append(idy)\n",
    "                    else:                       # Save if 1 S2 is KP\n",
    "                        idX.append(idx)                                      # 1: 1st best S2 image # 10: 2nd best S2 image \n",
    "                        idY.append(idy)                   \n",
    "        \n",
    "        # Store all pixel values (!=none) in an empty df            \n",
    "        rows = ['L'+str(index) for index in range(len(arr_temp))]\n",
    "        columns = [index for index in range(len(idX))]\n",
    "        results = pd.DataFrame(index=rows, columns=columns)\n",
    "        data = [] # It is recommended to collect data in a list of lists and then assign it to a df (Than modifying a df each iteration => time costly and prone to error of dtypes)\n",
    "        for idxLayer in range(len(arr_temp)):\n",
    "            pixelValues = Parallel(n_jobs=-1)(delayed(getPixelValue)(arr_temp,idxLayer,idX[k],idY[k]) for k in range(len(idX)))\n",
    "            data.append(pixelValues)\n",
    "        results = pd.DataFrame(data, index=rows, columns=columns).T\n",
    "        results.insert(loc=0, column='idx', value=idX)   # Add coordinates to df (while specifying position)\n",
    "        results.insert(loc=1, column='idy', value=idY)  \n",
    "        # Store all pixel values (==none) in an empty df            \n",
    "        results_none = pd.DataFrame({'idx_none':idX_none, 'idy_none':idY_none})\n",
    "        \n",
    "        # Export as excel files\n",
    "        os.makedirs(subdir2, exist_ok=True)\n",
    "        outputdir = os.path.join(subdir2, name+'.xlsx')\n",
    "        results.to_excel(outputdir, encoding='utf-8')\n",
    "        \n",
    "        outputdir2 = os.path.join(subdir2,'coordsNonePixelValues'+str(n)+'.xlsx') \n",
    "        results_none.to_excel(outputdir2, encoding='utf-8', index=False) # The coords of none pixel values are the same # save them 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T07:39:48.087769Z",
     "start_time": "2022-01-21T07:22:29.951882Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [17:18<00:00, 69.21s/it]\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# Prepare excel files containing all pixel values of best 2 S2 (including missing values = -99)\n",
    "# Seperate None and valid pixels into 2 excel files\n",
    "##########\n",
    "# With GF\n",
    "##########\n",
    "\n",
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\preparedInputData'\n",
    "gf_folders = ['withGF']\n",
    "\n",
    "for gf_folder in gf_folders:\n",
    "    subdir0 = os.path.join(maindir1,'withoutGF','France')    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    subdir2 = os.path.join(maindir2,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' in fileName]\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]  \n",
    "        arr0 = rio.open(os.path.join(subdir0,file1)).read()\n",
    "        img = rio.open(os.path.join(subdir1,file1)) # start by reading all layers\n",
    "        arr = img.read()\n",
    "        # Rank S2 scenes based on n° KP \n",
    "        indices = [i for i in range(1,36,9)]\n",
    "        nb_KP = []\n",
    "        for i in indices:\n",
    "            temp_copy = deepcopy(arr0[i])\n",
    "            temp_copy[temp_copy==-99]=9.96921e+36\n",
    "            nb_KP.append(len(np.argwhere(temp_copy<=1e+36).tolist()))        \n",
    "        df1 = pd.DataFrame({'indices':indices,'nb_KP':nb_KP})\n",
    "        df1.sort_values('nb_KP', inplace=True)  # order based on nb_KP and make changes to df permanent (order from worst to best)\n",
    "        df1.reset_index(drop=True, inplace=True) # Drop old index and make changes to df permanent\n",
    "        \n",
    "        # Select reflectance layers associated with best 2 images\n",
    "        name = 'Pixels_From_Best_2_S2_'+file1[7:15]\n",
    "        l = list(df1[2:]['indices']) # get the best 2 S2\n",
    "        # Create a new stacked array of layers to be used (from which we will extract coord of KP, UP, None pixels)\n",
    "        arr_temp = np.expand_dims(arr[0], axis=0)\n",
    "        for k1 in l:\n",
    "            for k2 in range(k1,k1+9):\n",
    "                arr_temp = np.append(arr_temp,np.expand_dims(arr[k2], axis=0),axis=0) # get 19 layers (1st layer is turbidity + 18 layers of best S2 images and associated combinations of bands )\n",
    "        \n",
    "        # Get all possible pixel coordinates for valid or none pixel values (=9.96921e+36)\n",
    "        idX = []\n",
    "        idY = [] \n",
    "        idX_none = []\n",
    "        idY_none = []\n",
    "        for idx in range(arr_temp.shape[1]):    # get all pixel coordinates\n",
    "            for idy in range(arr_temp.shape[2]):\n",
    "                if arr_temp[1,idx,idy] > 1e+36 or arr_temp[10,idx,idy] > 1e+36: # Exclude if pixel is none in one of the layers\n",
    "                    idX_none.append(idx)                                            \n",
    "                    idY_none.append(idy)\n",
    "                else:\n",
    "                    idX.append(idx)                                      # 1: 1st best S2 image # 10: 2nd best S2 image \n",
    "                    idY.append(idy)                   \n",
    "        \n",
    "        # Store all pixel values (!=none) in an empty df            \n",
    "        rows = ['L'+str(index) for index in range(len(arr_temp))]\n",
    "        columns = [index for index in range(len(idX))]\n",
    "        results = pd.DataFrame(index=rows, columns=columns)\n",
    "        data = [] # It is recommended to collect data in a list of lists and then assign it to a df (Than modifying a df each iteration => time costly and prone to error of dtypes)\n",
    "        for idxLayer in range(len(arr_temp)):\n",
    "            pixelValues = Parallel(n_jobs=-1)(delayed(getPixelValue)(arr_temp,idxLayer,idX[k],idY[k]) for k in range(len(idX)))\n",
    "            data.append(pixelValues)\n",
    "        results = pd.DataFrame(data, index=rows, columns=columns).T\n",
    "        results.insert(loc=0, column='idx', value=idX)   # Add coordinates to df (while specifying position)\n",
    "        results.insert(loc=1, column='idy', value=idY)  \n",
    "        # Store all pixel values (==none) in an empty df            \n",
    "        results_none = pd.DataFrame({'idx_none':idX_none, 'idy_none':idY_none})\n",
    "        \n",
    "        # Export as excel files\n",
    "        os.makedirs(subdir2, exist_ok=True)\n",
    "        outputdir = os.path.join(subdir2, name+'.xlsx')\n",
    "        results.to_excel(outputdir, encoding='utf-8')\n",
    "        \n",
    "        outputdir2 = os.path.join(subdir2,'coordsNonePixelValues'+str(n)+'.xlsx') \n",
    "        results_none.to_excel(outputdir2, encoding='utf-8', index=False) # The coords of none pixel values are the same # save them 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T08:01:31.554439Z",
     "start_time": "2022-01-21T07:39:54.184039Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [10:42<00:00, 42.82s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [10:55<00:00, 43.67s/it]\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# generate new turbidity maps #\n",
    "###############################\n",
    "\n",
    "# predict turbidity using all training dataset\n",
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\preparedInputData'\n",
    "maindir3 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "    \n",
    "for gf_folder in gf_folders:    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    subdir2 = os.path.join(maindir2,gf_folder,'France')\n",
    "    subdir3 = os.path.join(maindir3,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]        \n",
    "\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        for file2 in os.listdir(os.path.join(subdir1,file1)): \n",
    "            if 'Best_2' in file2:\n",
    "                ############### Read all training dataset (without splitting) ###############\n",
    "                # first train the model with the previously prepared training set. Then, apply the model to predict turbidity in whole study area #\n",
    "                excel_file = pd.read_excel(os.path.join(subdir1,file1,file2)) # step0: Read and split data\n",
    "                y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                features = ['L'+str(i) for i in range(1,len(excel_file.columns)-3)]\n",
    "                X = excel_file.values                \n",
    "#                 y = MinMaxScaler().fit_transform(y) # Data Normalization is not necessary for random forests\n",
    "#                 X = MinMaxScaler().fit_transform(X) # This will save us the time of invert normalization afterwards\n",
    "\n",
    "                Nfeatures = X.shape[1]\n",
    "                y = y.ravel() # flatten to 1d array # data is in a column format while it expected it in a row.\n",
    "                ############### Read all pixel values in 2 S2 images to predict corresponding turbidity values ###############\n",
    "                excel_file2 = pd.read_excel(os.path.join(subdir2,file2)) \n",
    "                idx = np.array(excel_file2['idx'].values,dtype=np.float).reshape(-1,1)                        \n",
    "                idy = np.array(excel_file2['idy'].values,dtype=np.float).reshape(-1,1)\n",
    "                excel_file2.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                features2 = ['L'+str(i) for i in range(1,len(excel_file2.columns)-3)]\n",
    "                S2_values = excel_file2.values                \n",
    "#                 S2_values = MinMaxScaler().fit_transform(S2_values) # Data Normalization\n",
    "                ############### Predict turbidity using RF ()############### \n",
    "                model = RFR(n_estimators=500, max_features=int(len(features)/3.0), max_depth=25, random_state=seed)                    \n",
    "                model.fit(X, y)\n",
    "                y_pred = model.predict(S2_values)\n",
    "                results = pd.DataFrame({'idx':idx.ravel(), 'idy':idy.ravel(), 'predTur':y_pred.ravel()})\n",
    "\n",
    "                # step7: Export as excel files\n",
    "                outputdir2 = os.path.join(subdir3,'Tur_'+file1+'.xlsx')\n",
    "                results.to_excel(outputdir2, encoding='utf-8', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T08:08:58.944764Z",
     "start_time": "2022-01-21T08:01:37.606147Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [03:47<00:00, 14.19s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [03:34<00:00, 13.39s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "    \n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    subdir2 = os.path.join(maindir2,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'None' in fileName]\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        # Read file: coordsNonePixelValues\n",
    "        excel_file1 = pd.read_excel(os.path.join(subdir1,file1)) # step0: Read and split data\n",
    "        idx_temp1 = list(excel_file1['idx_none'])\n",
    "        idy_temp1 = list(excel_file1['idy_none'])\n",
    "        noneValues = []\n",
    "        for i in range(excel_file1.shape[0]):\n",
    "            noneValues.append(9.96921e+36)\n",
    "        \n",
    "        df = pd.DataFrame({'idx':idx_temp1, 'idy':idy_temp1, 'predTur':noneValues})\n",
    "        # step7: Export as excel files\n",
    "        outputdir2 = os.path.join(subdir2,file1)\n",
    "        df.to_excel(outputdir2, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T08:19:39.548284Z",
     "start_time": "2022-01-21T08:09:04.967483Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [05:17<00:00, 21.17s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [05:16<00:00, 21.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# Read predicted turbidity pixel values and add the none values to it\n",
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "    \n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1, gf_folder, 'France')\n",
    "    subdir2 = os.path.join(maindir2, gf_folder, 'France')\n",
    "    files_temp1 = [fileName for fileName in os.listdir(subdir1) if ('Tur' in fileName)and('tiff' not in fileName)]  \n",
    "\n",
    "    for n in tqdm(range(len(files_temp1))):\n",
    "        file1 = files_temp1[n]\n",
    "        excel_file1 = pd.read_excel(os.path.join(subdir1,file1))\n",
    "        idx_temp1 = list(excel_file1['idx'])\n",
    "        idy_temp1 = list(excel_file1['idy'])\n",
    "        predTur_temp1 = list(excel_file1['predTur'])\n",
    "        \n",
    "        file2 = 'coordsNonePixelValues'+str(n)+'.xlsx'        \n",
    "        excel_file2 = pd.read_excel(os.path.join(subdir1,file2))        \n",
    "        idx_temp2 = list(excel_file2['idx'])\n",
    "        idy_temp2 = list(excel_file2['idy'])\n",
    "        predTur_temp2 = list(excel_file2['predTur'])\n",
    "        \n",
    "        idx = idx_temp1+idx_temp2\n",
    "        idy = idy_temp1+idy_temp2\n",
    "        predTur = predTur_temp1+predTur_temp2\n",
    "\n",
    "        results = pd.DataFrame({'idx':idx, 'idy':idy, 'predTur':predTur})\n",
    "        results.sort_values(by=['idx', 'idy'], ascending=True, inplace=True) # Sort Values by idx then by idy\n",
    "\n",
    "        rowsList = results['idx']\n",
    "        colList = results['idy']\n",
    "        turList = results['predTur']\n",
    "\n",
    "        file3 = file1[4:12]        \n",
    "        img = rio.open(os.path.join(subdir2,'merged_'+file3+'.tiff')) # start by reading all layers\n",
    "        arr = img.read()\n",
    "                \n",
    "        ######## Update Array ########  \n",
    "        # Export as images \n",
    "        temp_copy1 = deepcopy(arr[0]) # retain layer as actual turbidity\n",
    "        outputdir1 = os.path.join(subdir1, 'actual_'+file1[:-5]+'.tiff')\n",
    "        with rio.open(outputdir1,'w',driver='Gtiff', width=img.width, height=img.height, \n",
    "                            count=1,crs=img.crs,transform=img.transform, dtype='float32', nodata=9.96921e+36) as newImg:\n",
    "            newImg.write(temp_copy1,1)\n",
    "            newImg.close()\n",
    "        \n",
    "        temp_copy2 = deepcopy(arr[0]) # to be filled with predicted turbidity\n",
    "        for item in range(len(rowsList)):\n",
    "            temp_copy2[int(rowsList[item]),int(colList[item])] = turList[item]\n",
    "        outputdir2 = os.path.join(subdir1, 'predicted_'+file1[:-5]+'.tiff')\n",
    "        with rio.open(outputdir2,'w',driver='Gtiff', width=img.width, height=img.height, \n",
    "                            count=1,crs=img.crs,transform=img.transform, dtype='float32', nodata=9.96921e+36) as newImg:\n",
    "            newImg.write(temp_copy2,1)\n",
    "            newImg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate over/under estimation maps (for areas that have been gap filled display None)\n",
    "# in NTU \n",
    "# Need to exclude pixels where turbidity is none while S2 is known\n",
    "\n",
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\slope'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "\n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if ('tiff' in fileName) and ('actual' in fileName) ]\n",
    "    \n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        img = rio.open(os.path.join(subdir1,'actual_Tur_20190121.tiff'))        \n",
    "        file1 = files_temp[n]        \n",
    "        actual = rio.open(os.path.join(subdir1,file1)).read(1)\n",
    "        predicted = rio.open(os.path.join(subdir1,'predicted_'+file1[7:])).read(1)\n",
    "        \n",
    "        arrayBias = 100*(actual-predicted)/(actual+ EPSILON)\n",
    "        for i in range(actual.shape[0]):  # Exclude none values from this analysis\n",
    "            for j in range(actual.shape[1]):\n",
    "                if actual[i,j] > 1e+36 or predicted[i,j] > 1e+36:\n",
    "                    arrayBias[i,j] = actual[i,j]\n",
    "        \n",
    "        # Export as image\n",
    "        outputdir1 = os.path.join(maindir2,gf_folder,'France','biasArray'+file1[7:19]+'.tiff')\n",
    "        with rio.open(outputdir1,'w',driver='Gtiff', width=img.width, height=img.height, \n",
    "                            count=1,crs=img.crs,transform=img.transform, dtype='float32', nodata=9.96921e+36) as newImg:\n",
    "            newImg.write(arrayBias,1)\n",
    "            newImg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
