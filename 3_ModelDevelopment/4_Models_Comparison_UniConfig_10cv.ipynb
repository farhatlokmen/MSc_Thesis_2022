{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T04:24:25.362875Z",
     "start_time": "2022-01-16T04:24:21.280037Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(4)\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import rasterio as rio\n",
    "from copy import deepcopy\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam, Adam\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor as ANN\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNN\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.colors as colors\n",
    "from rasterio.plot import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T04:24:25.430130Z",
     "start_time": "2022-01-16T04:24:25.415111Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_random_seed(x):\n",
    "    tf.random.set_seed(x) # Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    np.random.seed(x)     # Set the `numpy` pseudo-random generator at a fixed value\n",
    "    random.seed(x)        # Set the `python` built-in pseudo-random generator at a fixed value      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T04:24:25.612869Z",
     "start_time": "2022-01-16T04:24:25.484062Z"
    }
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Reproducibility is a Problem when using parallel processing  (n_jobs = 1)#\n",
    "############################################################################ \n",
    "seed = 4\n",
    "set_random_seed(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T12:09:03.435942Z",
     "start_time": "2022-01-16T12:09:03.426961Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(learn_rate=0.01, units1=14,units2=12,activ_func1='sigmoid',activ_func2='sigmoid',activ_func3='sigmoid'):    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units1, kernel_initializer='uniform', activation=activ_func1, input_shape=(Nfeatures,))) \n",
    "    model.add(Dense(units2, kernel_initializer='uniform', activation=activ_func2))                           \n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation=activ_func3))\n",
    "    optimizer = Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adam\")\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-16T15:46:15.423771Z",
     "start_time": "2022-01-16T12:09:10.353468Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [01:33<00:00,  6.23s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 15/15 [56:39<00:00, 226.66s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 15/15 [2:38:48<00:00, 635.25s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning'\n",
    "ml_models = ['KNN','RF','ANN']\n",
    "# gf_folders = ['withoutGF','withGF']\n",
    "gf_folders = ['withGF']\n",
    "scoring = {'mse':'neg_mean_squared_error', 'r2': 'r2'}\n",
    "\n",
    "Model = []\n",
    "GF = []\n",
    "DATE = []\n",
    "Nb_S2_used = []\n",
    "mse = []\n",
    "r2 = []\n",
    "        \n",
    "for ml_model in ml_models:    \n",
    "    for gf_folder in gf_folders:    \n",
    "        subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "        files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]        \n",
    "        \n",
    "        for n in tqdm(range(len(files_temp))):\n",
    "            file1 = files_temp[n]\n",
    "            for file2 in os.listdir(os.path.join(subdir1,file1)): \n",
    "                if 'Best_4' not in file2:\n",
    "                    excel_file = pd.read_excel(os.path.join(subdir1,file1,file2)) # step0: Read and split data\n",
    "                    y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                    excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                    features = ['L'+str(i) for i in range(1,len(excel_file.columns)-3)]<\n",
    "                    X = excel_file.values                \n",
    "                    y = MinMaxScaler().fit_transform(y) # Data Normalization\n",
    "                    X = MinMaxScaler().fit_transform(X)\n",
    "                    train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                    kfold_indexes = list(KFold(10,shuffle=True,random_state=seed).split(train_X)) # split training into Kfolds and shuffle            \n",
    "                    Nfeatures = train_X.shape[1]\n",
    "                    ############### Model with selected hyper-parameters w/o cv (get scores using all data) (n_jobs=1 : to ensure replicability) ###############\n",
    "                    if ml_model == 'ANN':                    \n",
    "                        model = ANN(build_fn=build_model, epochs=100, batch_size=10, verbose=0)  # create model \n",
    "                    elif ml_model == 'KNN':                    \n",
    "                        train_y = train_y.ravel() # flatten to 1d array # data is in a column format while it expected it in a row.\n",
    "                        model = KNN(n_neighbors=8, leaf_size=1, weights='distance')\n",
    "                    elif ml_model == 'RF':\n",
    "                        train_y = train_y.ravel() # flatten to 1d array # data is in a column format while it expected it in a row.\n",
    "                        model = RFR(n_estimators=500, max_features=int(len(features)/3.0), max_depth=25, random_state=seed)\n",
    "\n",
    "                    scores = cross_validate(model,train_X,train_y,cv=kfold_indexes,scoring=scoring,return_estimator=True)            \n",
    "                    avg_mse = np.mean(scores['test_mse'])                \n",
    "                    avg_r2 = np.mean(scores['test_r2'])  \n",
    "                    ############### Save Results ###############\n",
    "                    Model.append(ml_model)\n",
    "                    GF.append(gf_folder)\n",
    "                    DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                    Nb_S2_used.append(int(file2[17:18]))\n",
    "                    mse.append(round(-avg_mse,4)) # the computed values are negative\n",
    "                    r2.append(round(avg_r2,4))    \n",
    "\n",
    "############### Export Results ###############                \n",
    "results = pd.DataFrame({'Model':Model,\n",
    "                        'Date':DATE,\n",
    "                        'Gapfilling':GF,\n",
    "                        'Nb_S2_used':Nb_S2_used,\n",
    "                        'mse':mse,\n",
    "                        'r2':r2\n",
    "                        })\n",
    "outputdir2 = os.path.join(maindir2,'10k_cvResults_all3Models.xlsx')\n",
    "results.to_excel(outputdir2, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select best learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T12:11:31.314335Z",
     "start_time": "2021-10-01T12:11:31.309350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Selected model: RF\n",
    "# N°S2: 2\n",
    "\n",
    "# run the algorithm on all training data\n",
    "# check MSE and R2 are similar to previous\n",
    "# Get feature importance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T04:47:21.191203Z",
     "start_time": "2022-01-17T04:44:34.265880Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [02:46<00:00, 11.10s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning'\n",
    "ml_models = ['RF']\n",
    "# gf_folders = ['withoutGF', 'withGF']\n",
    "gf_folders = ['withGF']\n",
    "\n",
    "Model = []\n",
    "GF = []\n",
    "DATE = []\n",
    "Nb_S2_used = []\n",
    "mse = []\n",
    "r2 = []\n",
    "best3Features = []\n",
    "        \n",
    "for ml_model in ml_models:    \n",
    "    for gf_folder in gf_folders:    \n",
    "        subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "        files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]        \n",
    "        \n",
    "        for n in tqdm(range(len(files_temp))):\n",
    "            file1 = files_temp[n]\n",
    "            for file2 in os.listdir(os.path.join(subdir1,file1)): \n",
    "                if 'Best_2' in file2:\n",
    "                    excel_file = pd.read_excel(os.path.join(subdir1,file1,file2)) # step0: Read and split data\n",
    "                    y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                    excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                    features = ['L'+str(i) for i in range(1,len(excel_file.columns)-3)]\n",
    "                    X = excel_file.values                \n",
    "                    y = MinMaxScaler().fit_transform(y) # Data Normalization\n",
    "                    X = MinMaxScaler().fit_transform(X)\n",
    "                    train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                    Nfeatures = train_X.shape[1]\n",
    "                    train_y = train_y.ravel() # flatten to 1d array # data is in a column format while it expected it in a row.\n",
    "                    \n",
    "                    ############### Predict turbidity using RF ()###############\n",
    "                    model = RFR(n_estimators=500, max_features=int(len(features)/3.0), max_depth=10, random_state=seed)                    \n",
    "                    model.fit(train_X, train_y)\n",
    "                    y_pred = model.predict(val_X)\n",
    "                    \n",
    "                    importance = model.feature_importances_\n",
    "                    indices = sorted(range(len(importance)), key=lambda i: importance[i])[-3:]\n",
    "                    dict_temp = {'B':[1,10,19,28],'G':[4,13,22,31],'R':[7,16,25,34],\n",
    "                                 'BG':[2,11,20,29],'GB':[5,14,23,32],'RB':[8,17,26,35],\n",
    "                                 'BR':[3,12,21,30],'GR':[6,15,24,33],'RG':[9,18,27,36]}\n",
    "                    best3F = ''\n",
    "                    for i in indices:\n",
    "                        for key,values in dict_temp.items():\n",
    "                            if i+1 in values: # Add 1 because the indices were counted from 0 whilst layer names start from L1\n",
    "                                best3F+=key+' '\n",
    "                    \n",
    "                    Model.append(ml_model)\n",
    "                    GF.append(gf_folder)\n",
    "                    DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                    Nb_S2_used.append(int(file2[17:18]))\n",
    "                    mse.append(round(mean_squared_error(val_y, y_pred, squared=True),4))\n",
    "                    r2.append(round(r2_score(val_y, y_pred),4)) \n",
    "                    best3Features.append(best3F)\n",
    "\n",
    "results = pd.DataFrame({'Model':Model,\n",
    "                        'Date':DATE,\n",
    "                        'Gapfilling':GF,\n",
    "                        'Nb_S2_used':Nb_S2_used,\n",
    "                        'mse':mse,\n",
    "                        'r2':r2,\n",
    "                        'best3Features':best3Features\n",
    "                        })\n",
    "\n",
    "# step7: Export as excel files\n",
    "outputdir2 = os.path.join(maindir2,'performanceResults_1Model.xlsx')\n",
    "results.to_excel(outputdir2, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T14:16:27.557130Z",
     "start_time": "2021-10-02T13:59:18.722682Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [10:16<00:00, 41.13s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [06:50<00:00, 27.39s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning'\n",
    "ml_models = ['RF']\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "\n",
    "Model = []\n",
    "GF = []\n",
    "DATE = []\n",
    "Nb_S2_used = []\n",
    "mse = []\n",
    "r2 = []\n",
    "        \n",
    "for ml_model in ml_models:    \n",
    "    for gf_folder in gf_folders:    \n",
    "        subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "        files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]        \n",
    "        \n",
    "        for n in tqdm(range(len(files_temp))):\n",
    "            file1 = files_temp[n]\n",
    "            for file2 in os.listdir(os.path.join(subdir1,file1)): \n",
    "                if 'Best_2' in file2:\n",
    "                    excel_file = pd.read_excel(os.path.join(subdir1,file1,file2)) # step0: Read and split data\n",
    "                    y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                    excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                    features = ['L'+str(i) for i in range(1,len(excel_file.columns)-3)]\n",
    "                    X = excel_file.values                \n",
    "                    y = MinMaxScaler().fit_transform(y) # Data Normalization\n",
    "                    X = MinMaxScaler().fit_transform(X)\n",
    "                    train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                    Nfeatures = train_X.shape[1]\n",
    "                    \n",
    "                    ############### Predict turbidity using ANN ()###############\n",
    "                    model = ANN(build_fn=build_model, epochs=100, batch_size=10, verbose=0)  # create model                    \n",
    "                    model.fit(train_X, train_y)\n",
    "                    y_pred = model.predict(val_X)\n",
    "                    \n",
    "                    Model.append(ml_model)\n",
    "                    GF.append(gf_folder)\n",
    "                    DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                    Nb_S2_used.append(int(file2[17:18]))\n",
    "                    mse.append(round(mean_squared_error(val_y, y_pred, squared=True),4))\n",
    "                    r2.append(round(r2_score(val_y, y_pred),4)) \n",
    "                    \n",
    "results = pd.DataFrame({'Model':Model,\n",
    "                        'Date':DATE,\n",
    "                        'Gapfilling':GF,\n",
    "                        'Nb_S2_used':Nb_S2_used,\n",
    "                        'mse':mse,\n",
    "                        'r2':r2\n",
    "                        })\n",
    "\n",
    "# step7: Export as excel files\n",
    "outputdir2 = os.path.join(maindir2,'perfANN.xlsx')\n",
    "results.to_excel(outputdir2, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T14:17:08.420840Z",
     "start_time": "2021-10-02T14:16:27.756629Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:20<00:00,  1.34s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:20<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning'\n",
    "ml_models = ['RF']\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "\n",
    "Model = []\n",
    "GF = []\n",
    "DATE = []\n",
    "Nb_S2_used = []\n",
    "mse = []\n",
    "r2 = []\n",
    "        \n",
    "for ml_model in ml_models:    \n",
    "    for gf_folder in gf_folders:    \n",
    "        subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "        files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]        \n",
    "        \n",
    "        for n in tqdm(range(len(files_temp))):\n",
    "            file1 = files_temp[n]\n",
    "            for file2 in os.listdir(os.path.join(subdir1,file1)): \n",
    "                if 'Best_2' in file2:\n",
    "                    excel_file = pd.read_excel(os.path.join(subdir1,file1,file2)) # step0: Read and split data\n",
    "                    y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                    excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                    features = ['L'+str(i) for i in range(1,len(excel_file.columns)-3)]\n",
    "                    X = excel_file.values                \n",
    "                    y = MinMaxScaler().fit_transform(y) # Data Normalization\n",
    "                    X = MinMaxScaler().fit_transform(X)\n",
    "                    train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=seed) # Split data\n",
    "\n",
    "                    Nfeatures = train_X.shape[1]\n",
    "                    train_y = train_y.ravel() # flatten to 1d array # data is in a column format while it expected it in a row.\n",
    "                    \n",
    "                    ############### Predict turbidity using KNN ()###############\n",
    "                    model = KNN(n_neighbors=8, leaf_size=1, weights='distance')\n",
    "                    model.fit(train_X, train_y)\n",
    "                    y_pred = model.predict(val_X)\n",
    "                    \n",
    "                    Model.append(ml_model)\n",
    "                    GF.append(gf_folder)\n",
    "                    DATE.append(file1[:4]+'-'+file1[4:6]+'-'+file1[6:])\n",
    "                    Nb_S2_used.append(int(file2[17:18]))\n",
    "                    mse.append(round(mean_squared_error(val_y, y_pred, squared=True),4))\n",
    "                    r2.append(round(r2_score(val_y, y_pred),4)) \n",
    "                    \n",
    "results = pd.DataFrame({'Model':Model,\n",
    "                        'Date':DATE,\n",
    "                        'Gapfilling':GF,\n",
    "                        'Nb_S2_used':Nb_S2_used,\n",
    "                        'mse':mse,\n",
    "                        'r2':r2\n",
    "                        })\n",
    "\n",
    "# step7: Export as excel files\n",
    "outputdir2 = os.path.join(maindir2,'perfKNN.xlsx')\n",
    "results.to_excel(outputdir2, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T12:24:03.443966Z",
     "start_time": "2021-10-01T12:24:03.292343Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR\n",
      "GB\n",
      "G\n"
     ]
    }
   ],
   "source": [
    "my_dir = r'G:\\MScThesis\\waterQualityMonitoring\\Results\\ML'\n",
    "excel_file = pd.read_excel(os.path.join(my_dir,'performanceResults_1Model.xlsx'))\n",
    "print(excel_file[excel_file['Gapfilling']=='withoutGF']['BestF1'].value_counts().idxmax()) # Most frequent \n",
    "print(excel_file[excel_file['Gapfilling']=='withoutGF']['BestF2'].value_counts().idxmax())\n",
    "print(excel_file[excel_file['Gapfilling']=='withoutGF']['BestF3'].value_counts().idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T12:24:04.263746Z",
     "start_time": "2021-10-01T12:24:04.246790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BG\n",
      "GB\n",
      "G\n"
     ]
    }
   ],
   "source": [
    "print(excel_file[excel_file['Gapfilling']=='withGF']['BestF1'].value_counts().idxmax()) # Most frequent \n",
    "print(excel_file[excel_file['Gapfilling']=='withGF']['BestF2'].value_counts().idxmax())\n",
    "print(excel_file[excel_file['Gapfilling']=='withGF']['BestF3'].value_counts().idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new turbidity maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare excel files containing all pixel values of best 2 S2 (including missing values = -99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T13:16:47.445091Z",
     "start_time": "2022-01-14T13:16:47.433637Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPixelValue(array,idx1,idx2,idx3):\n",
    "    return array[idx1,idx2,idx3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T13:16:48.124Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 15/15 [38:27<00:00, 153.86s/it]\n",
      " 47%|██████████████████████████████████████▋                                            | 7/15 [08:02<09:15, 69.43s/it]"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\preparedInputData'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "\n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    subdir2 = os.path.join(maindir2,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' in fileName]\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]        \n",
    "        img = rio.open(os.path.join(subdir1,file1)) # start by reading all layers\n",
    "        arr = img.read()\n",
    "        # Rank S2 scenes based on n° KP\n",
    "        indices = [i for i in range(1,36,9)]\n",
    "        nb_KP = []\n",
    "        for i in indices:\n",
    "            temp_copy = deepcopy(arr[i])\n",
    "            temp_copy[temp_copy==-99]=9.96921e+36\n",
    "            nb_KP.append(len(np.argwhere(temp_copy<=1e+36).tolist()))        \n",
    "        df1 = pd.DataFrame({'indices':indices,'nb_KP':nb_KP})\n",
    "        df1.sort_values('nb_KP', inplace=True)  # order based on nb_KP and make changes to df permanent (order from worst to best)\n",
    "        df1.reset_index(drop=True, inplace=True) # Drop old index and make changes to df permanent\n",
    "        # Select reflectance layers associated with best 2 images\n",
    "        name = 'Pixels_From_Best_2_S2_'+file1[7:15]\n",
    "        l = list(df1[2:]['indices']) # out of the 4 indices, removes the 1st two\n",
    "        # Create a new stacked array of layers to be used\n",
    "        arr_temp = np.expand_dims(arr[0], axis=0)\n",
    "        for k1 in l:\n",
    "            for k2 in range(k1,k1+9):\n",
    "                arr_temp = np.append(arr_temp,np.expand_dims(arr[k2], axis=0),axis=0) # get 19 layers (1st layer is turbidity + 18 layers of best S2 images and associated combinations of bands )\n",
    "\n",
    "        # Get all possible pixel coordinates for valid or none pixel values (=9.96921e+36)\n",
    "        idX = []\n",
    "        idY = [] \n",
    "        idX_none = []\n",
    "        idY_none = []\n",
    "        for idx in range(arr_temp.shape[1]):    # get all pixel coordinates\n",
    "            for idy in range(arr_temp.shape[2]):\n",
    "                if arr_temp[1,idx,idy] <= 1e+36 or arr_temp[10,idx,idy] <= 1e+36:  # if both s2 images have known values\n",
    "                    if arr_temp[1,idx,idy] != -99 or arr_temp[10,idx,idy] != -99:\n",
    "                        idX.append(idx)                                      # 1: 1st best S2 image # 10: 2nd best S2 image \n",
    "                        idY.append(idy)\n",
    "                    else:\n",
    "                        idX_none.append(idx)                                            \n",
    "                        idY_none.append(idy)\n",
    "                else:\n",
    "                    idX_none.append(idx)                                            \n",
    "                    idY_none.append(idy)                    \n",
    "        \n",
    "        # Store all pixel values (!=none) in an empty df            \n",
    "        rows = ['L'+str(index) for index in range(len(arr_temp))]\n",
    "        columns = [index for index in range(len(idX))]\n",
    "        results = pd.DataFrame(index=rows, columns=columns)\n",
    "        data = [] # It is recommended to collect data in a list of lists and then assign it to a df (Than modifying a df each iteration => time costly and prone to error of dtypes)\n",
    "        for idxLayer in range(len(arr_temp)):\n",
    "            pixelValues = Parallel(n_jobs=-1)(delayed(getPixelValue)(arr_temp,idxLayer,idX[k],idY[k]) for k in range(len(idX)))\n",
    "            data.append(pixelValues)\n",
    "        results = pd.DataFrame(data, index=rows, columns=columns).T\n",
    "        results.insert(loc=0, column='idx', value=idX)   # Add coordinates to df (while specifying position)\n",
    "        results.insert(loc=1, column='idy', value=idY)  \n",
    "        # Store all pixel values (==none) in an empty df            \n",
    "        results_none = pd.DataFrame({'idx_none':idX_none, 'idy_none':idY_none})\n",
    "        \n",
    "        # Export as excel files\n",
    "        os.makedirs(subdir2, exist_ok=True)\n",
    "        outputdir = os.path.join(subdir2, name+'.xlsx')\n",
    "        results.to_excel(outputdir, encoding='utf-8')\n",
    "        \n",
    "        outputdir2 = os.path.join(subdir2,'coordsNonePixelValues'+str(n)+'.xlsx') \n",
    "        results_none.to_excel(outputdir2, encoding='utf-8', index=False) # The coords of none pixel values are the same # save them 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T13:16:49.026Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# generate new turbidity maps #\n",
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T13:16:49.794Z"
    }
   },
   "outputs": [],
   "source": [
    "# predict turbidity using all training dataset\n",
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\preparedInputData'\n",
    "maindir3 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "    \n",
    "for gf_folder in gf_folders:    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    subdir2 = os.path.join(maindir2,gf_folder,'France')\n",
    "    subdir3 = os.path.join(maindir3,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'tiff' not in fileName]        \n",
    "\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        for file2 in os.listdir(os.path.join(subdir1,file1)): \n",
    "            if 'Best_2' in file2:\n",
    "                ############### Read all training dataset (without splitting) ###############\n",
    "                # first train the model with the previously prepared training set. Then, apply the model to predict turbidity in whole study area #\n",
    "                excel_file = pd.read_excel(os.path.join(subdir1,file1,file2)) # step0: Read and split data\n",
    "                y = np.array(excel_file['L0'].values,dtype=np.float).reshape(-1,1)                        # Target data\n",
    "                excel_file.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                features = ['L'+str(i) for i in range(1,len(excel_file.columns)-3)]\n",
    "                X = excel_file.values                \n",
    "#                 y = MinMaxScaler().fit_transform(y) # Data Normalization is not necessary for random forests\n",
    "#                 X = MinMaxScaler().fit_transform(X) # This will save us the time of invert normalization afterwards\n",
    "\n",
    "                Nfeatures = X.shape[1]\n",
    "                y = y.ravel() # flatten to 1d array # data is in a column format while it expected it in a row.\n",
    "                ############### Read all pixel values in 2 S2 images to predict corresponding turbidity values ###############\n",
    "                excel_file2 = pd.read_excel(os.path.join(subdir2,file2)) \n",
    "                idx = np.array(excel_file2['idx'].values,dtype=np.float).reshape(-1,1)                        \n",
    "                idy = np.array(excel_file2['idy'].values,dtype=np.float).reshape(-1,1)\n",
    "                excel_file2.drop(['Unnamed: 0','idx','idy','L0'], axis=1,inplace=True)\n",
    "                features2 = ['L'+str(i) for i in range(1,len(excel_file2.columns)-3)]\n",
    "                S2_values = excel_file2.values                \n",
    "#                 S2_values = MinMaxScaler().fit_transform(S2_values) # Data Normalization\n",
    "                ############### Predict turbidity using RF ()###############\n",
    "                model = KNN(n_neighbors=8, leaf_size=1, weights='distance')\n",
    "                    \n",
    "        \n",
    "                # model = RFR(n_estimators=500, max_features=int(len(features)/3.0), max_depth=10, random_state=seed)                    \n",
    "                model.fit(X, y)\n",
    "                y_pred = model.predict(S2_values)\n",
    "                results = pd.DataFrame({'idx':idx.ravel(), 'idy':idy.ravel(), 'predTur':y_pred.ravel()})\n",
    "\n",
    "                # step7: Export as excel files\n",
    "                outputdir2 = os.path.join(subdir3,'Tur_'+file1+'.xlsx')\n",
    "                results.to_excel(outputdir2, encoding='utf-8', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T13:16:51.523Z"
    }
   },
   "outputs": [],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\preparedInputData'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "    \n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    subdir2 = os.path.join(maindir2,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if 'None' in fileName]\n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        file1 = files_temp[n]\n",
    "        # Read file: coordsNonePixelValues\n",
    "        excel_file1 = pd.read_excel(os.path.join(subdir1,file1)) # step0: Read and split data\n",
    "        idx_temp1 = list(excel_file1['idx_none'])\n",
    "        idy_temp1 = list(excel_file1['idy_none'])\n",
    "        noneValues = []\n",
    "        for i in range(excel_file1.shape[0]):\n",
    "            noneValues.append(9.96921e+36)\n",
    "        \n",
    "        df = pd.DataFrame({'idx':idx_temp1, 'idy':idy_temp1, 'predTur':noneValues})\n",
    "        # step7: Export as excel files\n",
    "        outputdir2 = os.path.join(subdir2,file1)\n",
    "        df.to_excel(outputdir2, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T13:16:59.999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read predicted turbidity pixel values and add the none values to it\n",
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\preparedInputData'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "    \n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1, gf_folder, 'France')\n",
    "    subdir2 = os.path.join(maindir2, gf_folder, 'France')\n",
    "    files_temp1 = [fileName for fileName in os.listdir(subdir1) if ('Tur' in fileName)and('tiff' not in fileName)]  \n",
    "\n",
    "    for n in tqdm(range(len(files_temp1))):\n",
    "        file1 = files_temp1[n]\n",
    "        excel_file1 = pd.read_excel(os.path.join(subdir1,file1))\n",
    "        idx_temp1 = list(excel_file1['idx'])\n",
    "        idy_temp1 = list(excel_file1['idy'])\n",
    "        predTur_temp1 = list(excel_file1['predTur'])\n",
    "        \n",
    "        file2 = 'coordsNonePixelValues'+str(n)+'.xlsx'        \n",
    "        excel_file2 = pd.read_excel(os.path.join(subdir1,file2))        \n",
    "        idx_temp2 = list(excel_file2['idx'])\n",
    "        idy_temp2 = list(excel_file2['idy'])\n",
    "        predTur_temp2 = list(excel_file2['predTur'])\n",
    "        \n",
    "        idx = idx_temp1+idx_temp2\n",
    "        idy = idy_temp1+idy_temp2\n",
    "        predTur = predTur_temp1+predTur_temp2\n",
    "\n",
    "        results = pd.DataFrame({'idx':idx, 'idy':idy, 'predTur':predTur})\n",
    "        results.sort_values(by=['idx', 'idy'], ascending=True, inplace=True) # Sort Values by idx then by idy\n",
    "\n",
    "        rowsList = results['idx']\n",
    "        colList = results['idy']\n",
    "        turList = results['predTur']\n",
    "\n",
    "        file3 = file1[4:12]        \n",
    "        img = rio.open(os.path.join(subdir2,'merged_'+file3+'.tiff')) # start by reading all layers\n",
    "        arr = img.read()\n",
    "                \n",
    "        ######## Update Array ########  \n",
    "        # Export as images \n",
    "        temp_copy1 = deepcopy(arr[0]) # retain layer as actual turbidity\n",
    "        outputdir1 = os.path.join(subdir1, 'actual_'+file1[:-5]+'.tiff')\n",
    "        with rio.open(outputdir1,'w',driver='Gtiff', width=img.width, height=img.height, \n",
    "                            count=1,crs=img.crs,transform=img.transform, dtype='float32', nodata=9.96921e+36) as newImg:\n",
    "            newImg.write(temp_copy1,1)\n",
    "            newImg.close()\n",
    "        \n",
    "        temp_copy2 = deepcopy(arr[0]) # to be filled with predicted turbidity\n",
    "        for item in range(len(rowsList)):\n",
    "            temp_copy2[int(rowsList[item]),int(colList[item])] = turList[item]\n",
    "        outputdir2 = os.path.join(subdir1, 'predicted_'+file1[:-5]+'.tiff')\n",
    "        with rio.open(outputdir2,'w',driver='Gtiff', width=img.width, height=img.height, \n",
    "                            count=1,crs=img.crs,transform=img.transform, dtype='float32', nodata=9.96921e+36) as newImg:\n",
    "            newImg.write(temp_copy2,1)\n",
    "            newImg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T15:30:38.245628Z",
     "start_time": "2021-10-03T15:30:38.241637Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate over/under estimation maps (for areas that have been gap filled display None)\n",
    "# in NTU \n",
    "# Need to exclude pixels where turbidity is none while S2 is known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T05:00:17.698840Z",
     "start_time": "2022-01-08T04:59:39.456764Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████▋                           | 10/15 [00:13<00:06,  1.28s/it]<ipython-input-5-729171ef6556>:15: RuntimeWarning: overflow encountered in true_divide\n",
      "  arrayBias = (predicted-actual)/actual\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:19<00:00,  1.30s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:18<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\slope'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "\n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if ('tiff' in fileName) and ('actual' in fileName) ]\n",
    "    \n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        img = rio.open(os.path.join(subdir1,'actual_Tur_20190121.tiff'))        \n",
    "        file1 = files_temp[n]        \n",
    "        actual = rio.open(os.path.join(subdir1,file1)).read(1)\n",
    "        predicted = rio.open(os.path.join(subdir1,'predicted_'+file1[7:])).read(1)\n",
    "        \n",
    "        arrayBias = predicted-actual\n",
    "        for i in range(actual.shape[0]):  # Exclude none values from this analysis\n",
    "            for j in range(actual.shape[1]):\n",
    "                if actual[i,j] > 1e+36:\n",
    "                    arrayBias[i,j] = actual[i,j]\n",
    "        \n",
    "        # Export as image\n",
    "        outputdir1 = os.path.join(maindir2,gf_folder,'France','biasArray'+file1[7:19]+'.tiff')\n",
    "        with rio.open(outputdir1,'w',driver='Gtiff', width=img.width, height=img.height, \n",
    "                            count=1,crs=img.crs,transform=img.transform, dtype='float32', nodata=9.96921e+36) as newImg:\n",
    "            newImg.write(arrayBias,1)\n",
    "            newImg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T13:17:04.582Z"
    },
    "code_folding": [
     47,
     57
    ]
   },
   "outputs": [],
   "source": [
    "## https://gist.github.com/bshishov/5dc237f59f019b26145648e2124ca1c9\n",
    "\n",
    "EPSILON = 1e-10\n",
    "\n",
    "def _error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Simple error \"\"\"\n",
    "    return actual - predicted\n",
    "def _absolute_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" absolute error \"\"\"\n",
    "    return abs(actual - predicted)\n",
    "def _percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Percentage error\n",
    "    Note: result is NOT multiplied by 100\n",
    "    \"\"\"\n",
    "    return _error(actual, predicted) / (actual + EPSILON)\n",
    "def error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Simple error \"\"\"\n",
    "    return np.mean(_error(actual, predicted))\n",
    "def percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Percentage error\n",
    "    Note: result is NOT multiplied by 100\n",
    "    \"\"\"\n",
    "    return np.mean(_error(actual, predicted)/(actual + EPSILON))\n",
    "    \n",
    "def mse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Mean Squared Error \"\"\"\n",
    "    return np.mean(np.square(_error(actual, predicted)))\n",
    "def mdape(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Median Absolute Percentage Error\n",
    "    \"\"\"\n",
    "    return np.median(np.abs(_percentage_error(actual, predicted)))\n",
    "def R2_score(actual: np.ndarray, predicted: np.ndarray):\n",
    "    return r2_score(actual, predicted)\n",
    "\n",
    "METRICS = {\n",
    "    'mse': mse,\n",
    "    'mdape': mdape, # less affected by outliers\n",
    "    '_error':_error,\n",
    "    '_percentage_error':_percentage_error,\n",
    "    'error':error,\n",
    "    'percentage_error':percentage_error,\n",
    "    'R2_score':R2_score,\n",
    "}\n",
    "\n",
    "def evaluate(actual: np.ndarray, predicted: np.ndarray, metrics=('mse', 'mdape', '_error','_percentage_error', 'error','percentage_error','R2_score')):\n",
    "    results = {}\n",
    "    for name in metrics:\n",
    "        try:\n",
    "            results[name] = METRICS[name](actual, predicted)\n",
    "        except Exception as err:\n",
    "            results[name] = np.nan\n",
    "            print('Unable to compute metric {0}: {1}'.format(name, err))\n",
    "    return results\n",
    "\n",
    "def evaluate_all(actual: np.ndarray, predicted: np.ndarray):\n",
    "    return evaluate(actual, predicted, metrics=set(METRICS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T13:17:05.865Z"
    }
   },
   "outputs": [],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\slope'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "\n",
    "gf = []\n",
    "d = []\n",
    "err = []\n",
    "errP = []\n",
    "MdAPE = []\n",
    "MSE = []\n",
    "R2score = []\n",
    "\n",
    "for gf_folder in gf_folders:\n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if ('tiff' in fileName) and ('actual' in fileName) ]\n",
    "    \n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        img = rio.open(os.path.join(subdir1,'actual_Tur_20190121.tiff'))        \n",
    "        file1 = files_temp[n]        \n",
    "        actual = rio.open(os.path.join(subdir1,file1)).read(1)\n",
    "        predicted = rio.open(os.path.join(subdir1,'predicted_'+file1[7:])).read(1)\n",
    "\n",
    "        actualValues = []\n",
    "        predValues = []\n",
    "        for i in range(actual.shape[0]):  # Exclude none values from this analysis\n",
    "            for j in range(actual.shape[1]):\n",
    "                if actual[i,j]<10000 and actual[i,j]>-90 and predicted[i,j]<10000 and predicted[i,j]>-90:\n",
    "                    actualValues.append(actual[i,j])\n",
    "                    predValues.append(predicted[i,j])\n",
    "        # Use error metrics that do not penalize large differences between actual and predicted\n",
    "        errorMetrics = evaluate(np.array(actualValues,dtype=np.float64), np.array(predValues,dtype=np.float64), metrics=('error', 'percentage_error','mdape','mse','R2_score'))\n",
    "    \n",
    "        gf.append(gf_folder)\n",
    "        d.append(file1[11:19])\n",
    "        err.append(round(errorMetrics['error'],4))\n",
    "        errP.append(round(100*errorMetrics['percentage_error'],2))\n",
    "        MdAPE.append(round(100*errorMetrics['mdape'],2))\n",
    "        MSE.append(round(errorMetrics['mse'],4))\n",
    "        R2score.append(round(100*errorMetrics['R2_score'],2))\n",
    "df = pd.DataFrame({'gf':gf,'d':d,'err':err, 'errP':errP, 'MdAPE':MdAPE,'MSE':MSE,'R2score':R2score})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get n° pixels ranging between 0-10 / 10-30 / 30-max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-09T20:02:10.975469Z",
     "start_time": "2022-01-09T20:02:10.396841Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 55.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 57.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d</th>\n",
       "      <th>gf</th>\n",
       "      <th>a11</th>\n",
       "      <th>p11</th>\n",
       "      <th>a30</th>\n",
       "      <th>p30</th>\n",
       "      <th>amax</th>\n",
       "      <th>pmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190121</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>89155</td>\n",
       "      <td>100594</td>\n",
       "      <td>2217</td>\n",
       "      <td>2464</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190211</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>46189</td>\n",
       "      <td>101368</td>\n",
       "      <td>893</td>\n",
       "      <td>499</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190311</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>102720</td>\n",
       "      <td>103007</td>\n",
       "      <td>180</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190411</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>102516</td>\n",
       "      <td>103058</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190511</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>102419</td>\n",
       "      <td>103057</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20190611</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>102231</td>\n",
       "      <td>103058</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20190711</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>103052</td>\n",
       "      <td>103058</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20190811</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>102805</td>\n",
       "      <td>102805</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20190911</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>103057</td>\n",
       "      <td>103058</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20191011</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>7015</td>\n",
       "      <td>103058</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20191111</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>66679</td>\n",
       "      <td>85759</td>\n",
       "      <td>495</td>\n",
       "      <td>4631</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20191201</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>82621</td>\n",
       "      <td>79418</td>\n",
       "      <td>18290</td>\n",
       "      <td>21775</td>\n",
       "      <td>2057</td>\n",
       "      <td>1865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20200101</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>99950</td>\n",
       "      <td>100450</td>\n",
       "      <td>3058</td>\n",
       "      <td>2607</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20200211</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>99740</td>\n",
       "      <td>102765</td>\n",
       "      <td>298</td>\n",
       "      <td>293</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20200311</td>\n",
       "      <td>withoutGF</td>\n",
       "      <td>100391</td>\n",
       "      <td>100445</td>\n",
       "      <td>2666</td>\n",
       "      <td>2613</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20190121</td>\n",
       "      <td>withGF</td>\n",
       "      <td>89155</td>\n",
       "      <td>89224</td>\n",
       "      <td>2217</td>\n",
       "      <td>13834</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20190211</td>\n",
       "      <td>withGF</td>\n",
       "      <td>46189</td>\n",
       "      <td>80227</td>\n",
       "      <td>893</td>\n",
       "      <td>22831</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20190311</td>\n",
       "      <td>withGF</td>\n",
       "      <td>102720</td>\n",
       "      <td>102962</td>\n",
       "      <td>180</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20190411</td>\n",
       "      <td>withGF</td>\n",
       "      <td>102516</td>\n",
       "      <td>103058</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20190511</td>\n",
       "      <td>withGF</td>\n",
       "      <td>102419</td>\n",
       "      <td>103046</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20190611</td>\n",
       "      <td>withGF</td>\n",
       "      <td>102231</td>\n",
       "      <td>103058</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20190711</td>\n",
       "      <td>withGF</td>\n",
       "      <td>103052</td>\n",
       "      <td>103058</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20190811</td>\n",
       "      <td>withGF</td>\n",
       "      <td>102805</td>\n",
       "      <td>103058</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20190911</td>\n",
       "      <td>withGF</td>\n",
       "      <td>103057</td>\n",
       "      <td>103058</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20191011</td>\n",
       "      <td>withGF</td>\n",
       "      <td>7015</td>\n",
       "      <td>103058</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20191111</td>\n",
       "      <td>withGF</td>\n",
       "      <td>66679</td>\n",
       "      <td>39666</td>\n",
       "      <td>495</td>\n",
       "      <td>63391</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20191201</td>\n",
       "      <td>withGF</td>\n",
       "      <td>82621</td>\n",
       "      <td>82561</td>\n",
       "      <td>18290</td>\n",
       "      <td>18324</td>\n",
       "      <td>2057</td>\n",
       "      <td>2173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20200101</td>\n",
       "      <td>withGF</td>\n",
       "      <td>99950</td>\n",
       "      <td>101379</td>\n",
       "      <td>3058</td>\n",
       "      <td>1679</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20200211</td>\n",
       "      <td>withGF</td>\n",
       "      <td>99740</td>\n",
       "      <td>103010</td>\n",
       "      <td>298</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20200311</td>\n",
       "      <td>withGF</td>\n",
       "      <td>100391</td>\n",
       "      <td>103040</td>\n",
       "      <td>2666</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           d         gf     a11     p11    a30    p30  amax  pmax\n",
       "0   20190121  withoutGF   89155  100594   2217   2464     1     0\n",
       "1   20190211  withoutGF   46189  101368    893    499    14     0\n",
       "2   20190311  withoutGF  102720  103007    180     51     2     0\n",
       "3   20190411  withoutGF  102516  103058     12      0     2     0\n",
       "4   20190511  withoutGF  102419  103057     18      1     0     0\n",
       "5   20190611  withoutGF  102231  103058      0      0     4     0\n",
       "6   20190711  withoutGF  103052  103058      3      0     2     0\n",
       "7   20190811  withoutGF  102805  102805      3      0     6     0\n",
       "8   20190911  withoutGF  103057  103058      0      0     1     0\n",
       "9   20191011  withoutGF    7015  103058      0      0     0     0\n",
       "10  20191111  withoutGF   66679   85759    495   4631    13     3\n",
       "11  20191201  withoutGF   82621   79418  18290  21775  2057  1865\n",
       "12  20200101  withoutGF   99950  100450   3058   2607    11     0\n",
       "13  20200211  withoutGF   99740  102765    298    293     5     0\n",
       "14  20200311  withoutGF  100391  100445   2666   2613     1     0\n",
       "15  20190121     withGF   89155   89224   2217  13834     1     0\n",
       "16  20190211     withGF   46189   80227    893  22831    14     0\n",
       "17  20190311     withGF  102720  102962    180     96     2     0\n",
       "18  20190411     withGF  102516  103058     12      0     2     0\n",
       "19  20190511     withGF  102419  103046     18     12     0     0\n",
       "20  20190611     withGF  102231  103058      0      0     4     0\n",
       "21  20190711     withGF  103052  103058      3      0     2     0\n",
       "22  20190811     withGF  102805  103058      3      0     6     0\n",
       "23  20190911     withGF  103057  103058      0      0     1     0\n",
       "24  20191011     withGF    7015  103058      0      0     0     0\n",
       "25  20191111     withGF   66679   39666    495  63391    13     1\n",
       "26  20191201     withGF   82621   82561  18290  18324  2057  2173\n",
       "27  20200101     withGF   99950  101379   3058   1679    11     0\n",
       "28  20200211     withGF   99740  103010    298     48     5     0\n",
       "29  20200311     withGF  100391  103040   2666     18     1     0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\slope'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "\n",
    "gf = []\n",
    "d = []\n",
    "a11 = []\n",
    "a30 = []\n",
    "amax = []\n",
    "p11 = []\n",
    "p30 = []\n",
    "pmax = []\n",
    "\n",
    "for gf_folder in gf_folders:    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if ('tiff' in fileName) and ('actual' in fileName) ]\n",
    "    \n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        img = rio.open(os.path.join(subdir1,'actual_Tur_20190121.tiff'))        \n",
    "        file1 = files_temp[n]        \n",
    "        actual = rio.open(os.path.join(subdir1,file1)).read(1)\n",
    "        predicted = rio.open(os.path.join(subdir1,'predicted_'+file1[7:])).read(1)\n",
    "        \n",
    "        A11 = len(actual[actual<11])\n",
    "        A30 = len(actual[actual<30]) - len(actual[actual<11])\n",
    "        Amax = len(actual[actual<10000]) - len(actual[actual<30])\n",
    "        \n",
    "        P11 = len(predicted[predicted<11])\n",
    "        P30 = len(predicted[predicted<30]) - len(predicted[predicted<11])\n",
    "        Pmax = len(predicted[predicted<10000]) - len(predicted[predicted<30])\n",
    "        \n",
    "        gf.append(gf_folder)\n",
    "        d.append(file1[11:19])\n",
    "        \n",
    "        a11.append(A11)\n",
    "        a30.append(A30)\n",
    "        amax.append(Amax)\n",
    "        p11.append(P11)\n",
    "        p30.append(P30)\n",
    "        pmax.append(Pmax)\n",
    "        \n",
    "        \n",
    "df = pd.DataFrame({'d':d,'gf':gf,'a11':a11,'p11':p11,'a30':a30, 'p30':p30, 'amax':amax, 'pmax':pmax})\n",
    "df     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T08:21:59.316433Z",
     "start_time": "2022-01-12T08:20:49.050847Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:33<00:00,  2.25s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:35<00:00,  2.40s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gf</th>\n",
       "      <th>d</th>\n",
       "      <th>pixels11</th>\n",
       "      <th>pixels31</th>\n",
       "      <th>pixelsMax</th>\n",
       "      <th>N_pixels_changed</th>\n",
       "      <th>N_pixels_noChange</th>\n",
       "      <th>pixels_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190121</td>\n",
       "      <td>88944</td>\n",
       "      <td>2106</td>\n",
       "      <td>0</td>\n",
       "      <td>323</td>\n",
       "      <td>91050</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190211</td>\n",
       "      <td>45106</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1967</td>\n",
       "      <td>45129</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190311</td>\n",
       "      <td>102711</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>102753</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190411</td>\n",
       "      <td>102516</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>102516</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190511</td>\n",
       "      <td>102419</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>102420</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190611</td>\n",
       "      <td>102231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>102231</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190711</td>\n",
       "      <td>103052</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>103052</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190811</td>\n",
       "      <td>102552</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "      <td>102552</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20190911</td>\n",
       "      <td>103057</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>103057</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20191011</td>\n",
       "      <td>7015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7015</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20191111</td>\n",
       "      <td>66447</td>\n",
       "      <td>384</td>\n",
       "      <td>1</td>\n",
       "      <td>355</td>\n",
       "      <td>66832</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20191201</td>\n",
       "      <td>78449</td>\n",
       "      <td>17421</td>\n",
       "      <td>1530</td>\n",
       "      <td>5568</td>\n",
       "      <td>97400</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20200101</td>\n",
       "      <td>99827</td>\n",
       "      <td>2476</td>\n",
       "      <td>0</td>\n",
       "      <td>716</td>\n",
       "      <td>102303</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20200211</td>\n",
       "      <td>99720</td>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>99993</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>withoutGF</td>\n",
       "      <td>20200311</td>\n",
       "      <td>100325</td>\n",
       "      <td>2547</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>102872</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190121</td>\n",
       "      <td>77457</td>\n",
       "      <td>565</td>\n",
       "      <td>0</td>\n",
       "      <td>13351</td>\n",
       "      <td>78022</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190211</td>\n",
       "      <td>40803</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>6221</td>\n",
       "      <td>40875</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190311</td>\n",
       "      <td>102625</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "      <td>102626</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190411</td>\n",
       "      <td>102516</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>102516</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190511</td>\n",
       "      <td>102407</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>102407</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190611</td>\n",
       "      <td>102231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>102231</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190711</td>\n",
       "      <td>103052</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>103052</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190811</td>\n",
       "      <td>102805</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>102805</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20190911</td>\n",
       "      <td>103057</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>103057</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20191011</td>\n",
       "      <td>7015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7015</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20191111</td>\n",
       "      <td>25983</td>\n",
       "      <td>461</td>\n",
       "      <td>0</td>\n",
       "      <td>40743</td>\n",
       "      <td>26444</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20191201</td>\n",
       "      <td>69097</td>\n",
       "      <td>4671</td>\n",
       "      <td>1586</td>\n",
       "      <td>27614</td>\n",
       "      <td>75354</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20200101</td>\n",
       "      <td>98493</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>4306</td>\n",
       "      <td>98713</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20200211</td>\n",
       "      <td>99736</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>263</td>\n",
       "      <td>99780</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>withGF</td>\n",
       "      <td>20200311</td>\n",
       "      <td>100391</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2649</td>\n",
       "      <td>100409</td>\n",
       "      <td>103058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           gf         d  pixels11  pixels31  pixelsMax  N_pixels_changed  \\\n",
       "0   withoutGF  20190121     88944      2106          0               323   \n",
       "1   withoutGF  20190211     45106        23          0              1967   \n",
       "2   withoutGF  20190311    102711        42          0               149   \n",
       "3   withoutGF  20190411    102516         0          0                14   \n",
       "4   withoutGF  20190511    102419         1          0                17   \n",
       "5   withoutGF  20190611    102231         0          0                 4   \n",
       "6   withoutGF  20190711    103052         0          0                 5   \n",
       "7   withoutGF  20190811    102552         0          0               262   \n",
       "8   withoutGF  20190911    103057         0          0                 1   \n",
       "9   withoutGF  20191011      7015         0          0                 0   \n",
       "10  withoutGF  20191111     66447       384          1               355   \n",
       "11  withoutGF  20191201     78449     17421       1530              5568   \n",
       "12  withoutGF  20200101     99827      2476          0               716   \n",
       "13  withoutGF  20200211     99720       273          0                50   \n",
       "14  withoutGF  20200311    100325      2547          0               186   \n",
       "15     withGF  20190121     77457       565          0             13351   \n",
       "16     withGF  20190211     40803        72          0              6221   \n",
       "17     withGF  20190311    102625         1          0               276   \n",
       "18     withGF  20190411    102516         0          0                14   \n",
       "19     withGF  20190511    102407         0          0                30   \n",
       "20     withGF  20190611    102231         0          0                 4   \n",
       "21     withGF  20190711    103052         0          0                 5   \n",
       "22     withGF  20190811    102805         0          0                 9   \n",
       "23     withGF  20190911    103057         0          0                 1   \n",
       "24     withGF  20191011      7015         0          0                 0   \n",
       "25     withGF  20191111     25983       461          0             40743   \n",
       "26     withGF  20191201     69097      4671       1586             27614   \n",
       "27     withGF  20200101     98493       220          0              4306   \n",
       "28     withGF  20200211     99736        44          0               263   \n",
       "29     withGF  20200311    100391        18          0              2649   \n",
       "\n",
       "    N_pixels_noChange  pixels_total  \n",
       "0               91050        103058  \n",
       "1               45129        103058  \n",
       "2              102753        103058  \n",
       "3              102516        103058  \n",
       "4              102420        103058  \n",
       "5              102231        103058  \n",
       "6              103052        103058  \n",
       "7              102552        103058  \n",
       "8              103057        103058  \n",
       "9                7015        103058  \n",
       "10              66832        103058  \n",
       "11              97400        103058  \n",
       "12             102303        103058  \n",
       "13              99993        103058  \n",
       "14             102872        103058  \n",
       "15              78022        103058  \n",
       "16              40875        103058  \n",
       "17             102626        103058  \n",
       "18             102516        103058  \n",
       "19             102407        103058  \n",
       "20             102231        103058  \n",
       "21             103052        103058  \n",
       "22             102805        103058  \n",
       "23             103057        103058  \n",
       "24               7015        103058  \n",
       "25              26444        103058  \n",
       "26              75354        103058  \n",
       "27              98713        103058  \n",
       "28              99780        103058  \n",
       "29             100409        103058  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maindir1 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\predictedTurbidity'\n",
    "maindir2 = r'G:\\MScThesis\\waterQualityMonitoring\\Data\\MLearning\\massProduction\\slope'\n",
    "gf_folders = ['withoutGF', 'withGF']\n",
    "\n",
    "gf = []\n",
    "d = []\n",
    "pixels11 = []\n",
    "pixels31 = []\n",
    "pixelsMax = []\n",
    "N_pixels_changed = []\n",
    "N_pixels_noChange = []\n",
    "\n",
    "for gf_folder in gf_folders:    \n",
    "    subdir1 = os.path.join(maindir1,gf_folder,'France')\n",
    "    files_temp = [fileName for fileName in os.listdir(subdir1) if ('tiff' in fileName) and ('actual' in fileName) ]\n",
    "    \n",
    "    for n in tqdm(range(len(files_temp))):\n",
    "        img = rio.open(os.path.join(subdir1,'actual_Tur_20190121.tiff'))        \n",
    "        file1 = files_temp[n]        \n",
    "        actual = rio.open(os.path.join(subdir1,file1)).read(1)\n",
    "        predicted = rio.open(os.path.join(subdir1,'predicted_'+file1[7:])).read(1)\n",
    "        \n",
    "        gf.append(gf_folder)\n",
    "        d.append(file1[11:19])\n",
    "        \n",
    "        pixels_total = 0\n",
    "        noChangeinClass = 0\n",
    "        changeinClass = 0\n",
    "        \n",
    "        v11=0\n",
    "        v31=0\n",
    "        vmax=0\n",
    "        for i in range(actual.shape[0]):  # Exclude none values from this analysis\n",
    "            for j in range(actual.shape[1]):\n",
    "                if actual[i,j]<1e+7 and actual[i,j]>-90:\n",
    "                    pixels_total+=1\n",
    "                    actualValue = actual[i,j]\n",
    "                    predValue = predicted[i,j]\n",
    "                    if (actualValue<11 and predValue<11):\n",
    "                        v11+=1\n",
    "                        noChangeinClass+=1\n",
    "                    elif (actualValue<31 and predValue<31 and actualValue>=11 and predValue>=11):\n",
    "                        v31+=1\n",
    "                        noChangeinClass+=1\n",
    "                    elif (actualValue>=31 and predValue>=31):\n",
    "                        vmax+=1\n",
    "                        noChangeinClass+=1\n",
    "                    else:\n",
    "                        changeinClass+=1\n",
    "                    \n",
    "        pixels11.append(v11)\n",
    "        pixels31.append(v31)\n",
    "        pixelsMax.append(vmax)\n",
    "        N_pixels_changed.append(changeinClass)\n",
    "        N_pixels_noChange.append(noChangeinClass)\n",
    "    \n",
    "df = pd.DataFrame({'gf':gf,'d':d,'pixels11':pixels11,'pixels31':pixels31,'pixelsMax':pixelsMax,'N_pixels_changed':N_pixels_changed,'N_pixels_noChange':N_pixels_noChange,'pixels_total':pixels_total})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
